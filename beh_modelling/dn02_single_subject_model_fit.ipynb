{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioral modelling\n",
    "\n",
    "#### Assumptions:\n",
    "\n",
    "1. experienced utility is some function of displayed reward magnitude: \n",
    "    - simple model: $u(x) = x$ \n",
    "    - complex model: $u(x) = x^{\\delta} \\quad if \\quad x>0$ or $u(x) = -\\gamma |x|^{\\delta} \\quad if \\quad x>0$\n",
    "2. values (reflecting beliefs about probability) are learned with simple delta learning rule (TD model):\n",
    "    - $V(a_t)=V(a_{t-1})+ \\alpha [R-V(a_{t-i})]$\n",
    "3. choice probabilites are probabilistic functions of expected utilities:\n",
    "    - degenerate model: $p(a)=\\frac{EV(a)}{EV(a)+EV(b)}$ (parsimoneous approach introduced by *Summerfield et al. 2011*)\n",
    "    - full softmax model: $p(a)=\\frac{\\exp(\\theta EV(a))}{\\exp(\\theta EV(a)) + \\exp(\\theta EV(b))}$\n",
    "\n",
    "#### Free parameters:\n",
    "\n",
    "- $\\alpha \\in [0, 1]$: learning rate (modelling learning rate above half would result in model selecting previously rewarded / not punished option which is not realistic \n",
    "- $\\gamma \\in [0, \\infty]$: loss aversion parameter\n",
    "- $\\delta \\in [0, 1]$: risk aversion parameter\n",
    "- $\\theta \\in [0, \\infty]$: inverse temperature for softmax function\n",
    "\n",
    "#### Model variations:\n",
    "\n",
    "- model 1: (k=1 parameter) simple utility model, degenerate model for choice probabilities\n",
    "- model 2: (k=2 parameters) simple utility model, full softmax model\n",
    "- model 3: (k=4 parameters) complex utility model, full softmax model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_values(df, info, alpha):\n",
    "    '''Implements TD learning model on experienced probabilistic outcomes.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.Dataframe): clean behavioral responses\n",
    "        alpha (float): learning rate \n",
    "        \n",
    "    Returns:\n",
    "        val (np.array): reflects algorithm trialwise beliefs about \n",
    "            probabilities that box will be rewarded / punished\n",
    "    '''\n",
    "    \n",
    "    val = np.zeros((info['n_trials'], 2))\n",
    "    val[0] = [.5, .5] # Initial beliefs (agnostic)\n",
    "\n",
    "    for trial, rwd in df['rwd'][:-1].iteritems():\n",
    "        val[trial+1, 1] = val[trial, 1] + alpha * ((rwd + 1)/2 - val[trial, 1])\n",
    "        val[trial+1, 0] = val[trial, 0] + alpha * ((-rwd + 1)/2 - val[trial, 0])    \n",
    "\n",
    "    return val\n",
    "\n",
    "def estimate_utilities(df, info, gamma=1, delta=1):                                               \n",
    "    '''Implements function converting reward magnitude to experienced utility.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.Dataframe): clean behavioral responses\n",
    "        gamma (float): loss aversion parameter\n",
    "        delta: (float): risk aversion parameter\n",
    "        \n",
    "    Returns:\n",
    "        util (np.array): reflects algorithm trialwise estimates of utility \n",
    "            for both left and right boxes\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    util = np.zeros((info['n_trials'], 2))\n",
    "    \n",
    "    if info['condition'] == 'pun': \n",
    "        factor = (-1) * gamma\n",
    "    else:\n",
    "        factor = 1\n",
    "        \n",
    "    util[:, 0] = factor * np.power(np.abs(df['magn_left']), delta)\n",
    "    util[:, 1] = factor * np.power(np.abs(df['magn_right']), delta)\n",
    "\n",
    "    return util\n",
    "\n",
    "def estimate_choice_probability(df, val, util, kind='simple', theta=None):\n",
    "    '''Implements softmax decision rule reflecting choice probabilities'''\n",
    "\n",
    "    # Calculate expected value for both options\n",
    "    ev = np.multiply(util, val)\n",
    "    \n",
    "    if kind == 'simple':\n",
    "        p = ev / np.sum(ev, axis=1)[:, np.newaxis]\n",
    "        if np.sum(ev) < 0: \n",
    "            p = np.fliplr(p)\n",
    "        \n",
    "    elif kind == 'softmax':\n",
    "        p = np.exp(theta * ev) / np.sum(np.exp(theta * ev), axis=1)[:, np.newaxis]\n",
    "\n",
    "    return p\n",
    "\n",
    "def g_square(df, p):\n",
    "    '''Calculate badness-of-fit quality measure. G-square is inversely \n",
    "    related to log likelyhood.'''\n",
    "\n",
    "    ll = 0 \n",
    "\n",
    "    for i, resp in df['response'].iteritems():\n",
    "\n",
    "        if resp == -1:\n",
    "            ll += np.log(p[i, 0])\n",
    "        elif resp == 1:\n",
    "            ll += np.log(p[i, 1])\n",
    "\n",
    "    return (-2) * ll\n",
    "\n",
    "### Behavioral Models #######################################################\n",
    "def model1(df, info, alpha):\n",
    "    '''Simple one-parameter model with variable learning rate.'''\n",
    "    \n",
    "    val = estimate_values(df, info, alpha)\n",
    "    util = estimate_utilities(df, info)\n",
    "    p = estimate_choice_probability(df, val, util, kind='simple')\n",
    "    \n",
    "    return p\n",
    "    \n",
    "    \n",
    "def model2(df, info, alpha, theta):\n",
    "    '''Two-parameter model  with variable learning rate and inverse T.'''\n",
    "    \n",
    "    val = estimate_values(df, info, alpha)\n",
    "    util = estimate_utilities(df, info)\n",
    "    p = estimate_choice_probability(df, val, util, kind='softmax', theta=theta)\n",
    "    \n",
    "    return p\n",
    "\n",
    "def model3(df, info, alpha, theta, gamma, delta):\n",
    "    '''Four-parameter model.\n",
    "    \n",
    "    Params:\n",
    "        df (pd.Dataframe): clean behavioral responses\n",
    "        alpha (float): learning rate\n",
    "        theta (float): inverse softmax temperature\n",
    "        gamma (float): loss aversion\n",
    "        delta (float): risk aversion \n",
    "    '''\n",
    "    \n",
    "    val = estimate_values(df, info, alpha)\n",
    "    util = estimate_utilities(df, info, gamma, delta)\n",
    "    p = estimate_choice_probability(df, val, util, kind='softmax', theta=theta)\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show example model fit for models 1 and 2\n",
    "Models are fitted to subject responses using $G^2$ which is a measure of badness-of-fit derived from log likelyhood. In this section model 1 and 2 are fitted for all possible parameters sampled from parameter space. Both task conditions are fitted separately for single subject. Finally, $G^2$  function is visualised in the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_grid = 100\n",
    "\n",
    "alpha = np.linspace(0, 1, N_grid)\n",
    "theta = np.linspace(0, .5, N_grid)\n",
    "\n",
    "### Model 1 #################################################################\n",
    "fit1 = np.zeros((2, N_grid))\n",
    "\n",
    "for i, a in enumerate(alpha):\n",
    "    fit1[0, i] = g_square(df_rew, model1(df_rew, info_rew, a))\n",
    "    fit1[1, i] = g_square(df_pun, model1(df_pun, info_pun, a))\n",
    "    \n",
    "### Model 2 #################################################################\n",
    "av, tv = np.meshgrid(alpha, theta)\n",
    "\n",
    "fit2 = np.zeros((2, N_grid, N_grid))\n",
    "for i, a in enumerate(alpha):\n",
    "    for j, t in enumerate(theta):\n",
    "        fit2[0, i, j] = g_square(df_rew, model2(df_rew, info_rew, a, t))\n",
    "        fit2[1, i, j] = g_square(df_pun, model2(df_pun, info_pun, a, t))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-ticks')\n",
    "\n",
    "# Figure 1\n",
    "fig1, ax = plt.subplots(facecolor='w', figsize=(10, 5))\n",
    "ax.plot(alpha, fit1[0,:], linewidth=2, color='g', label='reward')\n",
    "ax.plot(alpha, fit1[1,:], linewidth=2, color='r', label='punishment')\n",
    "\n",
    "ax.set_ylabel('$G^2$')\n",
    "ax.set_xlabel('alpha')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Figure 2\n",
    "fig2, (ax2r, ax2p) = plt.subplots(nrows=1, ncols=2, facecolor='w', figsize=(10, 5))\n",
    "im2r = ax2r.contourf(alpha, theta, fit2[0].T, levels=50, cmap='Greens_r')\n",
    "im2p = ax2p.contourf(alpha, theta, fit2[1].T, levels=50, cmap='Reds_r')\n",
    "\n",
    "ax2r.set_xlabel('alpha')\n",
    "ax2r.set_ylabel('theta')\n",
    "ax2p.set_xlabel('alpha')\n",
    "ax2p.set_ylabel('theta')\n",
    "\n",
    "fig2.colorbar(im2r, ax=ax2r)\n",
    "fig2.colorbar(im2p, ax=ax2p)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting for single subject responses\n",
    "In this section model parameters are optimized for explaining subject responses. Behavioral responsed are pooled across both task conditions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['model1'] = {'k': 1, 'g_square': None, 'x': None}\n",
    "results['model2'] = {'k': 2, 'g_square': None, 'x': None}\n",
    "results['model3'] = {'k': 4, 'g_square': None, 'x': None}\n",
    "\n",
    "### Model 1 #################################################################\n",
    "bounds1 = Bounds([0], [1])\n",
    "\n",
    "def cost_model1(x):\n",
    "    '''Optimization function for model 1.'''\n",
    "    g_rew = g_square(df_rew, model1(df_rew, info_rew, x))\n",
    "    g_pun = g_square(df_pun, model1(df_pun, info_pun, x))\n",
    "    return g_rew  + g_pun\n",
    "\n",
    "x0 = np.array([.5])\n",
    "\n",
    "res1 = minimize(cost_model1, x0, method='SLSQP', bounds=bounds1)\n",
    "results['model1']['g_square'] = res1['fun']\n",
    "results['model1']['x'] = res1['x']\n",
    "\n",
    "### Model 2 #################################################################\n",
    "bounds2 = Bounds([0, 0], [1, np.inf])\n",
    "\n",
    "def cost_model2(x):\n",
    "    '''Optimization function for model 2.'''\n",
    "    g_rew = g_square(df_rew, model2(df_rew, info_rew, x[0], x[1]))\n",
    "    g_pun = g_square(df_pun, model2(df_pun, info_pun, x[0], x[1]))\n",
    "    return g_rew + g_pun\n",
    "\n",
    "x0 = np.array([.5, .5])\n",
    "\n",
    "res2 = minimize(cost_model2, x0, method='SLSQP', bounds=bounds2)    \n",
    "results['model2']['g_square'] = res2['fun']\n",
    "results['model2']['x'] = res2['x']\n",
    "\n",
    "### Model 3 #################################################################\n",
    "bounds3 = Bounds([0, 0, 0, 0], [1, np.inf, np.inf, 1])\n",
    "\n",
    "def cost_model3(x):\n",
    "    '''Optimization function for model 2.'''\n",
    "    g_rew = g_square(df_rew, model3(df_rew, info_rew, x[0], x[1], x[2], x[3]))\n",
    "    g_pun = g_square(df_pun, model3(df_pun, info_pun, x[0], x[1], x[2], x[3]))\n",
    "    return g_rew + g_pun\n",
    "\n",
    "x0 = np.array([.5, .5, 1, 1])\n",
    "\n",
    "res3 = minimize(cost_model3, x0, method='SLSQP', bounds=bounds3)    \n",
    "results['model3']['g_square'] = res3['fun'] \n",
    "results['model3']['x'] = res3['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model comparison for single subject\n",
    "In this section three models are compared with respect to their Akaike Information Criterion (AIC score). AIC enables comparision of models with different number of free parameters. AIC is defined as:\n",
    "$$AIC = G^2 + 2k$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aic = np.zeros(3)\n",
    "\n",
    "for i, model in enumerate(results):\n",
    "    aic[i] = results[model]['g_square'] + 2*results[model]['k']\n",
    "\n",
    "plt.style.use('seaborn-ticks')\n",
    "\n",
    "fig, ax = plt.subplots(facecolor='w', figsize=(10, 3))\n",
    "\n",
    "color = ['#01579B' for _ in range(3)]\n",
    "color[np.argmin(aic)] = '#A67C00'\n",
    "ax.barh(range(3), aic, color=color, alpha=.75)\n",
    "ax.set_xlabel('Akaike Information Criterion')\n",
    "ax.set_yticks(range(3))\n",
    "ax.set_yticklabels(['model1', 'model2', 'model3'])\n",
    "ax.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
