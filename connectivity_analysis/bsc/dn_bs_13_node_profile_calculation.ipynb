{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amber-running",
   "metadata": {},
   "source": [
    "# Node profile dissimilarity between conditions\n",
    "\n",
    "Aim of node profile dissimilarity analysis is to find network nodes (ROIs) with highest variability of module assignemnt between task conditions. Highly variable nodes has different node profies depending on the task condtion. **Node profile** for node $i$ is a single row / column of group level agreement matrix. $i$th element of node profile vector reflects probability that nodes $i$ and $j$ will be placed inside the same community in randomly selected individual network. Correlation between node profile vectors from different conditions are calculated to assess similairy between node profiles. Average between all six condition pairs (rew+ ↔ rew-, rew+ ↔ pun+, ...) is calculated as mean similarity. Since raw connectivity values are hard to interpret, z-score is calculated for all ROIs mean similarity. These values are stored in `dissim` vector. \n",
    "\n",
    "Lower `dissim` values indicate ROIs with most between-condition variability in node profile. Dissimilarity significance is calculated using Monte Carlo procedure. First, individual module assignemnt vectors are randomly shuffled. Then the same procedure is applied to calculate null distribution of dissimilaity: agreement is calculated for individual conditions, then for each ROI node profile vectors are correlated across conditions yielding dissimilarity values, dissimilarity values are averaged and z-scored. This procedure is repeated `n_reps` times. Entire procedure is applied to single gamma and independent on other gammas. \n",
    "\n",
    "Finally dissimilary p-values are FDR corrected to reveal (for each gamma) set of ROIs with significantly variable node profile. \n",
    "\n",
    "\n",
    "> **Analysis type**: Multiple γ (calculations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import combinations\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bct.algorithms.clustering import agreement\n",
    "from dn_utils.networks import zscore_vector\n",
    "from dn_utils.path import path\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = \"combined_roi\"\n",
    "gamma_range = np.arange(0.5, 2.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_corrmats = join(path[\"bsc\"], \"corrmats\")\n",
    "path_corrmats_unthr = join(path_corrmats, atlas, \"unthr\")\n",
    "\n",
    "m = {}\n",
    "for gamma in gamma_range:\n",
    "    gamma_str = str(float(gamma)).replace(\".\", \"_\")\n",
    "    path_corrmats_unthr_gamma = join(path_corrmats_unthr, f\"gamma_{gamma_str}\")\n",
    "    m[gamma] = np.load(join(path_corrmats_unthr_gamma, \"m_aggregated.npy\"))\n",
    "    \n",
    "# Load subject exclusion\n",
    "df_exclusion = pd.read_csv(join(path[\"nistats\"], \"exclusion/exclusion.csv\"), \n",
    "                           index_col=0)\n",
    "ok_index = df_exclusion[\"ok_all\"]\n",
    "\n",
    "# Load ROI table\n",
    "df_roi = pd.read_csv(join(path_corrmats, atlas, \"roi_table_filtered.csv\"))\n",
    "\n",
    "# Meta information about corrmats dimensions\n",
    "with open(join(path_corrmats, atlas, \"corrmats_aggregated.json\"), \"r\") as f:\n",
    "    corrmats_meta = json.loads(f.read()) \n",
    "    \n",
    "n_subjects_ok = sum(ok_index)\n",
    "n_conditions = len(corrmats_meta[\"dim2\"])\n",
    "n_perr_sign = len(corrmats_meta[\"dim3\"])\n",
    "n_roi = len(corrmats_meta[\"dim4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_rowwise(arr1, arr2):\n",
    "    \"\"\"Calculate correlations between corresponding rows of two arrays.\"\"\"\n",
    "    n = len(arr1)\n",
    "    return np.diag(np.corrcoef(arr1, arr2)[:n, n:])\n",
    "\n",
    "def shuffle_along_axis(a, axis):\n",
    "    \"\"\"Shuffle array along specific axis.\"\"\"\n",
    "    idx = np.random.rand(*a.shape).argsort(axis=axis)\n",
    "    return np.take_along_axis(a,idx,axis=axis)\n",
    "\n",
    "def calculate_dissimiliarty(m):\n",
    "    \"\"\"...\"\"\"\n",
    "    # Condition dependent agreements\n",
    "    d_rew_inc = agreement(m[:, 0, 0].T)\n",
    "    d_rew_dec = agreement(m[:, 0, 1].T)\n",
    "    d_pun_inc = agreement(m[:, 1, 0].T)\n",
    "    d_pun_dec = agreement(m[:, 1, 1].T)\n",
    "    \n",
    "    for d in [d_rew_inc, d_rew_dec, d_pun_inc, d_pun_dec]:\n",
    "        np.fill_diagonal(d, n_subjects_ok)\n",
    "\n",
    "    # All combinations\n",
    "    dissim = np.zeros((m.shape[-1]))\n",
    "    for d1, d2 in combinations([d_rew_inc, d_rew_dec, d_pun_inc, d_pun_dec], 2):\n",
    "        dissim = dissim + corr_rowwise(d1, d2)\n",
    "    dissim = dissim / 6\n",
    "    \n",
    "    # Between prediction errors\n",
    "    dissim_perr = np.zeros((m.shape[-1]))\n",
    "    dissim_perr = dissim_perr + corr_rowwise(d_rew_inc, d_rew_dec)\n",
    "    dissim_perr = dissim_perr + corr_rowwise(d_pun_inc, d_pun_dec)\n",
    "    dissim_perr = dissim_perr / 2\n",
    "    \n",
    "    # Between conditions\n",
    "    dissim_con = np.zeros((m.shape[-1]))\n",
    "    dissim_con = dissim_con + corr_rowwise(d_rew_inc, d_pun_inc)\n",
    "    dissim_con = dissim_con + corr_rowwise(d_rew_dec, d_pun_dec)\n",
    "    dissim_con = dissim_con / 2\n",
    "    \n",
    "    return dissim, dissim_perr, dissim_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nulls = 10_000\n",
    "np.random.seed(0)\n",
    "\n",
    "for gamma in gamma_range:\n",
    "    print(f\"γ = {gamma}\")\n",
    "\n",
    "    gamma_str = str(float(gamma)).replace(\".\", \"_\")\n",
    "    path_corrmats_unthr_gamma = join(path_corrmats_unthr, f\"gamma_{gamma_str}\")\n",
    "    mt = m[gamma][ok_index]\n",
    "\n",
    "    # Real dissimilarity values\n",
    "    dissim, dissim_perr, dissim_con = calculate_dissimiliarty(mt)\n",
    "    dissim_zscore = zscore_vector(dissim) \n",
    "    dissim_perr_zscore = zscore_vector(dissim_perr) \n",
    "    dissim_con_zscore = zscore_vector(dissim_con) \n",
    "    \n",
    "    # Monte-Carlo distribution of dissimilarity z-scores\n",
    "    dissim_null = np.zeros((n_nulls, n_roi))\n",
    "    dissim_perr_null = np.zeros((n_nulls, n_roi))\n",
    "    dissim_con_null = np.zeros((n_nulls, n_roi))\n",
    "    for rep in tqdm(range(n_nulls)):\n",
    "        m_null = shuffle_along_axis(mt, 3)\n",
    "        tmp_dissim, tmp_dissim_perr, tmp_dissim_con = calculate_dissimiliarty(m_null)\n",
    "        dissim_null[rep] = zscore_vector(tmp_dissim)\n",
    "        dissim_perr_null[rep] = zscore_vector(tmp_dissim_perr)\n",
    "        dissim_con_null[rep] = zscore_vector(tmp_dissim_con)\n",
    "\n",
    "    # Calculate significance\n",
    "    pval = np.mean(dissim_zscore > dissim_null, axis=0)\n",
    "    pval_perr = np.mean(dissim_perr_zscore > dissim_perr_null, axis=0)\n",
    "    pval_con = np.mean(dissim_con_zscore > dissim_con_null, axis=0)\n",
    "\n",
    "    # Save values\n",
    "    df_dissim = df_roi.copy()\n",
    "    df_dissim[f\"dissim_{gamma_str}\"] = dissim\n",
    "    df_dissim[f\"dissim_perr_{gamma_str}\"] = dissim_perr\n",
    "    df_dissim[f\"dissim_con_{gamma_str}\"] = dissim_con\n",
    "    \n",
    "    df_dissim[f\"dissim_zscore_{gamma_str}\"] = dissim_zscore\n",
    "    df_dissim[f\"dissim_perr_zscore_{gamma_str}\"] = dissim_perr_zscore\n",
    "    df_dissim[f\"dissim_con_zscore_{gamma_str}\"] = dissim_con_zscore\n",
    "    \n",
    "    df_dissim[f\"pval_unc_{gamma_str}\"] = pval\n",
    "    df_dissim[f\"pval_perr_unc_{gamma_str}\"] = pval_perr\n",
    "    df_dissim[f\"pval_con_unc_{gamma_str}\"] = pval_con\n",
    "    \n",
    "    df_dissim[f\"pval_fdr_{gamma_str}\"] = fdrcorrection(pval)[1]\n",
    "    df_dissim[f\"pval_perr_fdr_{gamma_str}\"] = fdrcorrection(pval_perr)[1]\n",
    "    df_dissim[f\"pval_con_fdr_{gamma_str}\"] = fdrcorrection(pval_con)[1]\n",
    "    \n",
    "    df_dissim.to_csv(join(path_corrmats_unthr_gamma, \n",
    "                          \"node_profile_dissimilarity.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
