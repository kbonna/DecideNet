{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioral modelling\n",
    "\n",
    "#### Assumptions:\n",
    "\n",
    "1. experienced utility is some function of displayed reward magnitude: \n",
    "    - simple model: $u(x) = x$ \n",
    "    - complex model: $u(x) = x^{\\delta} \\quad if \\quad x>0$ or $u(x) = -\\gamma |x|^{\\delta} \\quad if \\quad x>0$\n",
    "2. values (reflecting beliefs about probability) are learned with simple delta learning rule (TD model):\n",
    "    - $V(a_t)=V(a_{t-1})+ \\alpha [R-V(a_{t-i})]$\n",
    "3. choice probabilites are probabilistic functions of expected utilities:\n",
    "    - degenerate model: $p(a)=\\frac{EV(a)}{EV(a)+EV(b)}$ (parsimoneous approach introduced by *Summerfield et al. 2011*)\n",
    "    - full softmax model: $p(a)=\\frac{\\exp(\\beta EV(a))}{\\exp(\\beta EV(a)) + \\exp(\\beta EV(b))}$\n",
    "\n",
    "#### Free parameters:\n",
    "\n",
    "- $\\alpha \\in [0, 1]$: learning rate (modelling learning rate above half would result in model selecting previously rewarded / not punished option which is not realistic \n",
    "- $\\gamma \\in [0, \\infty]$: loss aversion parameter\n",
    "- $\\delta \\in [0, 1]$: risk aversion parameter\n",
    "- $\\beta \\in [0, \\infty]$: inverse temperature for softmax function\n",
    "\n",
    "#### Model variations:\n",
    "\n",
    "- model 1: (k=1 parameter) simple utility model, degenerate model for choice probabilities\n",
    "- model 2: (k=2 parameters) simple utility model, full softmax model\n",
    "- model 3: (k=4 parameters) complex utility model, full softmax model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/kmb/Desktop/Neuroscience/Projects/BONNA_decide_net/code')\n",
    "from dn_utils.behavioral_models import *\n",
    "\n",
    "# plt.style.use('seaborn-ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of beh array: (32, 2, 110, 21)\n",
      "Conditions [(0, 'rew'), (1, 'pun')]\n",
      "Columns: [(0, 'block'), (1, 'rwd'), (2, 'magn_left'), (3, 'magn_right'), (4, 'response'), (5, 'rt'), (6, 'won_bool'), (7, 'won_magn'), (8, 'acc_after_trial'), (9, 'onset_iti'), (10, 'onset_iti_plan'), (11, 'onset_iti_glob'), (12, 'onset_dec'), (13, 'onset_dec_plan'), (14, 'onset_dec_glob'), (15, 'onset_isi'), (16, 'onset_isi_plan'), (17, 'onset_isi_glob'), (18, 'onset_out'), (19, 'onset_out_plan'), (20, 'onset_out_glob')]\n"
     ]
    }
   ],
   "source": [
    "beh_path = \"/home/kmb/Desktop/Neuroscience/Projects/BONNA_decide_net/\" \\\n",
    "           \"data/main_fmri_study/sourcedata/behavioral\" \n",
    "beh, meta = load_behavioral_data(root=beh_path)\n",
    "n_subjects = beh.shape[0]\n",
    "n_conditions = beh.shape[1]\n",
    "n_trials = beh.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show example model fit for models 1 and 2\n",
    "Models are fitted to subject responses using $G^2$ which is a measure of badness-of-fit derived from log likelyhood. In this section model 1 and 2 are fitted for all possible parameters sampled from parameter space. Both task conditions are fitted separately for single subject. Finally, $G^2$  function is visualised in the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_grid = 100\n",
    "subject = 1\n",
    "\n",
    "### Model 1 #################################################################\n",
    "alpha = np.linspace(0, 1, N_grid)\n",
    "\n",
    "fit1 = np.zeros((2, N_grid))\n",
    "for i, a in enumerate(alpha):\n",
    "    fit1[0, i] = g_square(beh, meta, subject, 0, \n",
    "                          model1(beh, meta, subject, 0, a)[2])\n",
    "    fit1[1, i] = g_square(beh, meta, subject, 1, \n",
    "                          model1(beh, meta, subject, 1, a)[2])\n",
    "    \n",
    "    \n",
    "# Figure 1\n",
    "fig1, ax = plt.subplots(facecolor='w', figsize=(10, 5))\n",
    "ax.plot(alpha, fit1[0,:], linewidth=2, color='g', label='reward')\n",
    "ax.plot(alpha, fit1[1,:], linewidth=2, color='r', label='punishment')\n",
    "\n",
    "ax.set_ylabel('$G^2$')\n",
    "ax.set_xlabel('alpha')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_grid = 100\n",
    "subject = 27\n",
    "\n",
    "## Model 2 #################################################################\n",
    "alpha = np.linspace(0, .5, N_grid)\n",
    "theta = np.linspace(0, .5, N_grid)\n",
    "\n",
    "av, tv = np.meshgrid(alpha, theta)\n",
    "\n",
    "fit2 = np.zeros((2, N_grid, N_grid))\n",
    "for i, a in enumerate(alpha):\n",
    "    for j, t in enumerate(theta):\n",
    "        fit2[0, i, j] = g_square(beh, meta, subject, 0,\n",
    "                                 model2(beh, meta, subject, 0, a, t)[2])\n",
    "        fit2[1, i, j] = g_square(beh, meta, subject, 1,\n",
    "                                 model2(beh, meta, subject, 1, a, t)[2])\n",
    "\n",
    "# Figure 2\n",
    "fig2, (ax2r, ax2p) = plt.subplots(nrows=1, ncols=2, \n",
    "                                  facecolor='w', figsize=(10, 5))\n",
    "im2r = ax2r.contourf(alpha, theta, fit2[0].T, levels=50, cmap='Greens_r')\n",
    "im2p = ax2p.contourf(alpha, theta, fit2[1].T, levels=50, cmap='Reds_r')\n",
    "\n",
    "ax2r.set_xlabel('alpha')\n",
    "ax2r.set_ylabel('theta')\n",
    "ax2p.set_xlabel('alpha')\n",
    "ax2p.set_ylabel('theta')\n",
    "\n",
    "fig2.colorbar(im2r, ax=ax2r)\n",
    "fig2.colorbar(im2p, ax=ax2p)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting for single subject responses\n",
    "In this section model parameters are optimized for explaining subject responses. Behavioral responsed are pooled across both task conditions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds1 = Bounds([0], [1])\n",
    "x0 = np.array([.5])\n",
    "\n",
    "def cost_model1(x):\n",
    "    '''Optimization function for model 1.'''\n",
    "    global subject\n",
    "    g_rew = g_square(beh, meta, subject, 0, model1(beh, meta, subject, 0, x)[2])\n",
    "    g_pun = g_square(beh, meta, subject, 1, model1(beh, meta, subject, 1, x)[2])\n",
    "    return g_rew + g_pun\n",
    "\n",
    "params_1 = np.zeros((n_subjects, 2)) # store results\n",
    "\n",
    "for subject in range(n_subjects):\n",
    "\n",
    "    print(f'Finding parameter for subject {subject}...')\n",
    "    res1 = minimize(cost_model1, x0, method='SLSQP', bounds=bounds1)\n",
    "    params_1[subject, 0] = res1['x']\n",
    "    params_1[subject, 1] = res1['fun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds2 = Bounds([0, 0], [1, np.inf])\n",
    "x0 = np.array([.5, 1])\n",
    "\n",
    "def cost_model2(x):\n",
    "    '''Optimization function for model 2.'''\n",
    "    g_rew = g_square(beh, meta, subject, 0, \n",
    "                     model2(beh, meta, subject, 0, x[0], x[1])[2])\n",
    "    g_pun = g_square(beh, meta, subject, 1,\n",
    "                     model2(beh, meta, subject, 0, x[0], x[1])[2])\n",
    "    return g_rew\n",
    "\n",
    "params_2 = np.zeros((n_subjects, 3)) # store results\n",
    "\n",
    "for subject in range(n_subjects):\n",
    "\n",
    "    res2 = minimize(cost_model2, x0, method='SLSQP', bounds=bounds2)\n",
    "    params_2[subject][0] = res2['x'][0] # alpha\n",
    "    params_2[subject][1] = res2['x'][1] # theta\n",
    "    params_2[subject][2] = res2['fun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, facecolor='w')\n",
    "\n",
    "sp = ax.scatter(params_2[:,0], params_2[:, 1], c=params_2[:, 2])\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('theta')\n",
    "\n",
    "plt.colorbar(sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model comparison for single subject\n",
    "In this section three models are compared with respect to their Akaike Information Criterion (AIC score). AIC enables comparision of models with different number of free parameters. AIC is defined as:\n",
    "$$AIC = G^2 + 2k$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 1\n",
    "\n",
    "results = {}\n",
    "results['model1'] = {'k': 1, 'g_square': None, 'x': None}\n",
    "results['model2'] = {'k': 2, 'g_square': None, 'x': None}\n",
    "results['model3'] = {'k': 4, 'g_square': None, 'x': None}\n",
    "\n",
    "### Model 1 #################################################################\n",
    "bounds1 = Bounds([0], [1])\n",
    "\n",
    "def cost_model1(x):\n",
    "    '''Optimization function for model 1.'''\n",
    "    g_rew = g_square(beh, meta, subject, 0, model1(beh, meta, subject, 0, x))\n",
    "    g_pun = g_square(beh, meta, subject, 1, model1(beh, meta, subject, 1, x))\n",
    "    return g_rew  + g_pun\n",
    "\n",
    "x0 = np.array([.5])\n",
    "\n",
    "res1 = minimize(cost_model1, x0, method='SLSQP', bounds=bounds1)\n",
    "results['model1']['g_square'] = res1['fun']\n",
    "results['model1']['x'] = res1['x']\n",
    "\n",
    "### Model 2 #################################################################\n",
    "bounds2 = Bounds([0, 0], [1, np.inf])\n",
    "\n",
    "def cost_model2(x):\n",
    "    '''Optimization function for model 2.'''\n",
    "    g_rew = g_square(beh, meta, subject, 0, \n",
    "                     model2(beh, meta, subject, 0, x[0], x[1]))\n",
    "    g_pun = g_square(beh, meta, subject, 1, \n",
    "                     model2(beh, meta, subject, 1, x[0], x[1]))\n",
    "    return g_rew + g_pun\n",
    "\n",
    "x0 = np.array([.5, 1])\n",
    "\n",
    "res2 = minimize(cost_model2, x0, method='SLSQP', bounds=bounds2)    \n",
    "results['model2']['g_square'] = res2['fun']\n",
    "results['model2']['x'] = res2['x']\n",
    "\n",
    "# ### Model 3 #################################################################\n",
    "bounds3 = Bounds([0, 0, 0, 0], [1, np.inf, np.inf, 1])\n",
    "\n",
    "def cost_model3(x):\n",
    "    '''Optimization function for model 3.'''\n",
    "    g_rew = g_square(beh, meta, subject, 0, \n",
    "                     model3(beh, meta, subject, 0, x[0], x[1], x[2], x[3]))\n",
    "    g_pun = g_square(beh, meta, subject, 1, \n",
    "                     model3(beh, meta, subject, 1, x[0], x[1], x[2], x[3]))\n",
    "    return g_rew + g_pun\n",
    "\n",
    "x0 = np.array([.5, .5, 1, 1])\n",
    "\n",
    "res3 = minimize(cost_model3, x0, method='SLSQP', bounds=bounds3)    \n",
    "results['model3']['g_square'] = res3['fun'] \n",
    "results['model3']['x'] = res3['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aic = np.zeros(3)\n",
    "\n",
    "for i, model in enumerate(results):\n",
    "    aic[i] = results[model]['g_square'] + 2*results[model]['k']\n",
    "\n",
    "plt.style.use('seaborn-ticks')\n",
    "\n",
    "fig, ax = plt.subplots(facecolor='w', figsize=(10, 3))\n",
    "\n",
    "color = ['#01579B' for _ in range(3)]\n",
    "color[np.argmin(aic)] = '#A67C00'\n",
    "ax.barh(range(3), aic, color=color, alpha=.75)\n",
    "ax.set_xlabel('Akaike Information Criterion')\n",
    "ax.set_yticks(range(3))\n",
    "ax.set_yticklabels(['model1', 'model2', 'model3'])\n",
    "ax.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
