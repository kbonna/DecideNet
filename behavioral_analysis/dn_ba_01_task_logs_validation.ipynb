{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating PRL task logs\n",
    "\n",
    "This script validates raw task logs organized acording to BIDS scheme. Script features:\n",
    "- checks proper folders and logs naming \n",
    "- looks for missing and extra files\n",
    "- looks for missing data within logs and removes duplicates\n",
    "\n",
    "---\n",
    "**Last update**: 07.01.2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-m02  sub-m06  sub-m10  sub-m14  sub-m18  sub-m22  sub-m26  sub-m30\n",
      "sub-m03  sub-m07  sub-m11  sub-m15  sub-m19  sub-m23  sub-m27  sub-m31\n",
      "sub-m04  sub-m08  sub-m12  sub-m16  sub-m20  sub-m24  sub-m28  sub-m32\n",
      "sub-m05  sub-m09  sub-m13  sub-m17  sub-m21  sub-m25  sub-m29  sub-m33\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import filecmp\n",
    "import pandas as pd\n",
    "\n",
    "path_root = '/home/kmb/Desktop/Neuroscience/Projects/BONNA_decide_net/'\n",
    "path_logs = os.path.join(\n",
    "    path_root, \n",
    "    'data/main_fmri_study/sourcedata/behavioral/task_logs'\n",
    ")\n",
    "\n",
    "!ls $path_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All logs for single subject sits within `<path_logs>/sub-<subject_label>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> No errors in folder naming.\n",
      "--> Subjects found: \n",
      "['m02', 'm03', 'm04', 'm05', 'm06', 'm07', 'm08', 'm09', 'm10', 'm11', 'm12', 'm13', 'm14', 'm15', 'm16', 'm17', 'm18', 'm19', 'm20', 'm21', 'm22', 'm23', 'm24', 'm25', 'm26', 'm27', 'm28', 'm29', 'm30', 'm31', 'm32', 'm33']\n",
      "--> Total number of subjects: 32\n"
     ]
    }
   ],
   "source": [
    "# Check valid folder names and infer available subjects\n",
    "subjects = []\n",
    "for subfolder in os.listdir(path_logs):\n",
    "    if subfolder[:5] != 'sub-m': \n",
    "        raise NameError(f'wrong folder name: {subfolder}')\n",
    "    if len(subfolder) != 7: \n",
    "        raise NameError(f'wrong folder name: {subfolder}')\n",
    "    subjects.append(subfolder[-3:])\n",
    "\n",
    "print(f'--> No errors in folder naming.')\n",
    "print(f'--> Subjects found: \\n{sorted(subjects)}')\n",
    "print(f'--> Total number of subjects: {len(subjects)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing files, extra files, spellings \n",
    "Use this code to find missing files or extra filels and resolve potential problems **manually** (because of small sample size and large variety of potential problem with files: spellings, procedure failures etc.). Rerun after fix, and proceed to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Logs not found: \n",
      "[]\n",
      "--> Total number of missing logs: 0\n",
      "--> Extra files: \n",
      "[]\n",
      "--> Number of subs with extra files: 0\n"
     ]
    }
   ],
   "source": [
    "missing, extra = [], []\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    path_subfolder = os.path.join(path_logs, 'sub-' + subject)\n",
    "    log_rew = f'{subject}_prl_DecideNet_rew.csv'\n",
    "    log_pun = f'{subject}_prl_DecideNet_pun.csv'\n",
    "    \n",
    "    # look for missing files\n",
    "    if log_rew not in os.listdir(path_subfolder):\n",
    "        missing.append(log_rew)\n",
    "    if log_pun not in os.listdir(path_subfolder):\n",
    "        missing.append(log_pun)\n",
    "\n",
    "    # look for extra files\n",
    "    if len(os.listdir(path_subfolder)) != 8:\n",
    "        extra.append([subject, len(os.listdir(path_subfolder))])\n",
    "        \n",
    "print(f'--> Logs not found: \\n{sorted(missing)}')\n",
    "print(f'--> Total number of missing logs: {len(missing)}')\n",
    "print(f'--> Extra files: \\n{extra}')\n",
    "print(f'--> Number of subs with extra files: {len(extra)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing data within files\n",
    "Use this code to find if there is missing data within logs (missing trials or missing columns). One can also look for variability in file size to detect potential problems. Finally, code looks for failed duplicate files generated by PsychoPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Logs with wrong shape: \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "proper_shape = (110, 28) # log dataframe correct size\n",
    "wrong_shape, file_size, wrong_duplicates = [], [], []\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    path_subfolder = os.path.join(path_logs, 'sub-' + subject)\n",
    "    log_rew_path = f'{path_subfolder}/{subject}_prl_DecideNet_rew.csv'\n",
    "    log_pun_path = f'{path_subfolder}/{subject}_prl_DecideNet_pun.csv'\n",
    "    df_rew = pd.read_csv(log_rew_path)\n",
    "    df_pun = pd.read_csv(log_pun_path)\n",
    "    \n",
    "    # Test log shape \n",
    "    if df_pun.shape != proper_shape:\n",
    "        wrong_shape.append([log_pun_path, df_pun.shape])\n",
    "    if df_rew.shape != proper_shape:\n",
    "        wrong_shape.append([log_rew_path, df_rew.shape])\n",
    "    \n",
    "    # Save file size\n",
    "    file_size.append((log_rew_path, os.path.getsize(log_pun_path)))\n",
    "    file_size.append((log_pun_path, os.path.getsize(log_pun_path)))\n",
    "    \n",
    "    # Look if duplicates are same\n",
    "    log_rew_path2 = f'{path_subfolder}/{subject}_prl_DecideNet_rew_2.csv'\n",
    "    log_pun_path2 = f'{path_subfolder}/{subject}_prl_DecideNet_pun_2.csv'\n",
    "    \n",
    "    if not filecmp.cmp(log_rew_path, log_rew_path2):\n",
    "        wrong_duplicates.append(log_rew_path)\n",
    "    if not filecmp.cmp(log_pun_path, log_pun_path2):\n",
    "        wrong_duplicates.append(log_pun_path)\n",
    "        \n",
    "print(f'--> Logs with wrong shape: \\n{wrong_shape}')\n",
    "# print(f'--> Logs with wrong duplicates: \\n{wrong_duplicates}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix subject_id field and remove spurious columns\n",
    "Apply only after manually resolved conflicts with file names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    \n",
    "    path_subfolder = os.path.join(path_logs, 'sub-' + subject)\n",
    "    log_rew_path = f'{path_subfolder}/{subject}_prl_DecideNet_rew.csv'\n",
    "    log_pun_path = f'{path_subfolder}/{subject}_prl_DecideNet_pun.csv'\n",
    "    df_rew = pd.read_csv(log_rew_path)\n",
    "    df_pun = pd.read_csv(log_pun_path)\n",
    "    \n",
    "    # Filter only useful columns\n",
    "    keys = ['block', 'rwd', 'magn_left', 'magn_right', 'onset_iti_plan',\n",
    "       'onset_isi_plan', 'onset_dec_plan', 'onset_out_plan', '.thisRepN',\n",
    "       '.thisTrialN', '.thisN', '.thisIndex', 'onset_iti', 'onset_iti_glob',\n",
    "       'onset_dec', 'onset_dec_glob', 'onset_isi', 'onset_isi_glob',\n",
    "       'onset_out', 'onset_out_glob', 'acc_after_trial', 'won_bool',\n",
    "       'won_magn', 'rt', 'response', 'subject_id', 'condition', 'group']\n",
    "    df_rew = df_rew[keys]\n",
    "    df_pun = df_pun[keys]\n",
    "    \n",
    "    # Fix subject_id field\n",
    "    df_rew['subject_id'] = subject\n",
    "    df_pun['subject_id'] = subject\n",
    "\n",
    "    # Save changes\n",
    "    df_rew.to_csv(log_rew_path, index=False)\n",
    "    df_pun.to_csv(log_pun_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
