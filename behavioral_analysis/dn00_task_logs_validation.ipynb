{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script takes as input path to raw task logs organized acording to BIDS scheme: all logs for single subject sits within `path_logs_raw/sub-subject_label`. Then validity of organization is checked, all missing files and duplicates are reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-m02  sub-m06  sub-m10  sub-m14  sub-m18  sub-m22  sub-m26  sub-m30\r\n",
      "sub-m03  sub-m07  sub-m11  sub-m15  sub-m19  sub-m23  sub-m27  sub-m31\r\n",
      "sub-m04  sub-m08  sub-m12  sub-m16  sub-m20  sub-m24  sub-m28  sub-m32\r\n",
      "sub-m05  sub-m09  sub-m13  sub-m17  sub-m21  sub-m25  sub-m29  sub-m33\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path_logs = \"/home/kmb/Desktop/Neuroscience/Projects/BONNA_decide_net/data/main_fmri_study/task_logs\"\n",
    "\n",
    "!ls $path_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Subject folders ##############\n",
      "--> No errors in folder naming.\n",
      "--> Subjects found: \n",
      "['m02', 'm03', 'm04', 'm05', 'm06', 'm07', 'm08', 'm09', 'm10', 'm11', 'm12', 'm13', 'm14', 'm15', 'm16', 'm17', 'm18', 'm19', 'm20', 'm21', 'm22', 'm23', 'm24', 'm25', 'm26', 'm27', 'm28', 'm29', 'm30', 'm31', 'm32', 'm33']\n",
      "--> Total number of subjects: 32\n",
      "############################################\n"
     ]
    }
   ],
   "source": [
    "# Check valid folder names and infer available subjects\n",
    "subjects = []\n",
    "for subfolder in os.listdir(path_logs):\n",
    "    if subfolder[:5] != 'sub-m': \n",
    "        raise NameError(f'wrong folder name: {subfolder}')\n",
    "    if len(subfolder) != 7: \n",
    "        raise NameError(f'wrong folder name: {subfolder}')\n",
    "    subjects.append(subfolder[-3:])\n",
    "\n",
    "print(f'############# Subject folders ##############')\n",
    "print(f'--> No errors in folder naming.')\n",
    "print(f'--> Subjects found: \\n{sorted(subjects)}')\n",
    "print(f'--> Total number of subjects: {len(subjects)}')\n",
    "print(f'############################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing files, extra files, spellings \n",
    "Use this code to find missing files or extra filels and resolve potential problems **manually** (because of small sample size and large variety of potential problem with files: spellings, procedure failures etc.). Rerun after fix, and proceed to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## Missing logs ################\n",
      "--> Logs not found: \n",
      "[]\n",
      "--> Total number of missing logs: 0\n",
      "--> Extra files: \n",
      "[]\n",
      "--> Number of subs with extra files: 0\n",
      "############################################\n"
     ]
    }
   ],
   "source": [
    "missing, extra = [], []\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    subfolder_path = os.path.join(path_logs, 'sub-' + subject)\n",
    "    log_rew = f'{subject}_prl_DecideNet_rew.csv'\n",
    "    log_pun = f'{subject}_prl_DecideNet_pun.csv'\n",
    "    \n",
    "    # look for missing files\n",
    "    if log_rew not in os.listdir(subfolder_path):\n",
    "        missing.append(log_rew)\n",
    "    if log_pun not in os.listdir(subfolder_path):\n",
    "        missing.append(log_pun)\n",
    "\n",
    "    # look for extra files\n",
    "    if len(os.listdir(subfolder_path)) != 8:\n",
    "        extra.append([subject, len(os.listdir(subfolder_path))])\n",
    "        \n",
    "        \n",
    "print(f'############## Missing logs ################')\n",
    "print(f'--> Logs not found: \\n{sorted(missing)}')\n",
    "print(f'--> Total number of missing logs: {len(missing)}')\n",
    "print(f'--> Extra files: \\n{extra}')\n",
    "print(f'--> Number of subs with extra files: {len(extra)}')\n",
    "print(f'############################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix subject_id field and remove spurious columns\n",
    "Apply only after manually resolved conflicts with file names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    \n",
    "    subfolder_path = os.path.join(path_logs, 'sub-' + subject)\n",
    "    log_rew_path = f'{subfolder_path}/{subject}_prl_DecideNet_rew.csv'\n",
    "    log_pun_path = f'{subfolder_path}/{subject}_prl_DecideNet_pun.csv'\n",
    "    df_rew = pd.read_csv(log_rew_path)\n",
    "    df_pun = pd.read_csv(log_pun_path)\n",
    "    \n",
    "    # Filter only useful columns\n",
    "    keys = ['block', 'rwd', 'magn_left', 'magn_right', 'onset_iti_plan',\n",
    "       'onset_isi_plan', 'onset_dec_plan', 'onset_out_plan', '.thisRepN',\n",
    "       '.thisTrialN', '.thisN', '.thisIndex', 'onset_iti', 'onset_iti_glob',\n",
    "       'onset_dec', 'onset_dec_glob', 'onset_isi', 'onset_isi_glob',\n",
    "       'onset_out', 'onset_out_glob', 'acc_after_trial', 'won_bool',\n",
    "       'won_magn', 'rt', 'response', 'subject_id', 'condition', 'group']\n",
    "    df_rew = df_rew[keys]\n",
    "    df_pun = df_pun[keys]\n",
    "    \n",
    "    # Fix subject_id field\n",
    "    df_rew['subject_id'] = subject\n",
    "    df_pun['subject_id'] = subject\n",
    "\n",
    "    # Save changes\n",
    "    df_rew.to_csv(log_rew_path, index=False)\n",
    "    df_pun.to_csv(log_pun_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing data within files\n",
    "Use this code to find if there is missing data within logs (missing trials or missing columns). One can also look for variability in file size to detect potential problems. Finally, code looks for failed duplicate files generated by PsychoPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## Missing data ################\n",
      "--> Logs with wrong shape: \n",
      "[]\n",
      "--> Logs with wrong duplicates: \n",
      "[]\n",
      "############################################\n"
     ]
    }
   ],
   "source": [
    "proper_shape = (110, 29) # log dataframe correct size\n",
    "wrong_shape, file_size, wrong_duplicates = [], [], []\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    subfolder_path = os.path.join(path_logs, 'sub-' + subject)\n",
    "    log_rew_path = f'{subfolder_path}/{subject}_prl_DecideNet_rew.csv'\n",
    "    log_pun_path = f'{subfolder_path}/{subject}_prl_DecideNet_pun.csv'\n",
    "    df_rew = pd.read_csv(log_rew_path)\n",
    "    df_pun = pd.read_csv(log_pun_path)\n",
    "    \n",
    "    # Test log shape \n",
    "    if df_pun.shape != proper_shape:\n",
    "        wrong_shape.append([log_pun_path, df_pun.shape])\n",
    "    if df_rew.shape != proper_shape:\n",
    "        wrong_shape.append([log_rew_path, df_rew.shape])\n",
    "    \n",
    "    # Save file size\n",
    "    file_size.append((log_rew_path, os.path.getsize(log_pun_path)))\n",
    "    file_size.append((log_pun_path, os.path.getsize(log_pun_path)))\n",
    "    \n",
    "    # Look if duplicates are same\n",
    "    log_rew_path2 = f'{subfolder_path}/{subject}_prl_DecideNet_rew_2.csv'\n",
    "    log_pun_path2 = f'{subfolder_path}/{subject}_prl_DecideNet_pun_2.csv'\n",
    "    \n",
    "    if not filecmp.cmp(log_rew_path, log_rew_path2):\n",
    "        wrong_duplicates.append(log_rew_path)\n",
    "    if not filecmp.cmp(log_pun_path, log_pun_path2):\n",
    "        wrong_duplicates.append(log_pun_path)\n",
    "        \n",
    "print(f'############## Missing data ################')\n",
    "print(f'--> Logs with wrong shape: \\n{wrong_shape}')\n",
    "print(f'--> Logs with wrong duplicates: \\n{wrong_duplicates}')\n",
    "print(f'############################################')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
