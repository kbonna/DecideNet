{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script takes as input path to raw task logs organized acording to BIDS scheme: all logs for single subject sits within `path_logs_raw/sub-subject_label`. Then validity of organization is checked, all missing files and duplicates are reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import filecmp\n",
    "import pandas as pd\n",
    "\n",
    "path_logs = \"/home/kmb/Desktop/Neuroscience/Projects/BONNA_decide_net/data/main_fmri_study/behavioral/task_logs\"\n",
    "\n",
    "!ls $path_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check valid folder names and infer available subjects\n",
    "subjects = []\n",
    "for subfolder in os.listdir(path_logs):\n",
    "    if subfolder[:5] != 'sub-m': \n",
    "        raise NameError(f'wrong folder name: {subfolder}')\n",
    "    if len(subfolder) != 7: \n",
    "        raise NameError(f'wrong folder name: {subfolder}')\n",
    "    subjects.append(subfolder[-3:])\n",
    "\n",
    "print(f'--> No errors in folder naming.')\n",
    "print(f'--> Subjects found: \\n{sorted(subjects)}')\n",
    "print(f'--> Total number of subjects: {len(subjects)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing files, extra files, spellings \n",
    "Use this code to find missing files or extra filels and resolve potential problems **manually** (because of small sample size and large variety of potential problem with files: spellings, procedure failures etc.). Rerun after fix, and proceed to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing, extra = [], []\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    subfolder_path = os.path.join(path_logs, 'sub-' + subject)\n",
    "    log_rew = f'{subject}_prl_DecideNet_rew.csv'\n",
    "    log_pun = f'{subject}_prl_DecideNet_pun.csv'\n",
    "    \n",
    "    # look for missing files\n",
    "    if log_rew not in os.listdir(subfolder_path):\n",
    "        missing.append(log_rew)\n",
    "    if log_pun not in os.listdir(subfolder_path):\n",
    "        missing.append(log_pun)\n",
    "\n",
    "    # look for extra files\n",
    "    if len(os.listdir(subfolder_path)) != 8:\n",
    "        extra.append([subject, len(os.listdir(subfolder_path))])\n",
    "        \n",
    "print(f'--> Logs not found: \\n{sorted(missing)}')\n",
    "print(f'--> Total number of missing logs: {len(missing)}')\n",
    "print(f'--> Extra files: \\n{extra}')\n",
    "print(f'--> Number of subs with extra files: {len(extra)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing data within files\n",
    "Use this code to find if there is missing data within logs (missing trials or missing columns). One can also look for variability in file size to detect potential problems. Finally, code looks for failed duplicate files generated by PsychoPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_shape = (110, 28) # log dataframe correct size\n",
    "wrong_shape, file_size, wrong_duplicates = [], [], []\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    subfolder_path = os.path.join(path_logs, 'sub-' + subject)\n",
    "    log_rew_path = f'{subfolder_path}/{subject}_prl_DecideNet_rew.csv'\n",
    "    log_pun_path = f'{subfolder_path}/{subject}_prl_DecideNet_pun.csv'\n",
    "    df_rew = pd.read_csv(log_rew_path)\n",
    "    df_pun = pd.read_csv(log_pun_path)\n",
    "    \n",
    "    # Test log shape \n",
    "    if df_pun.shape != proper_shape:\n",
    "        wrong_shape.append([log_pun_path, df_pun.shape])\n",
    "    if df_rew.shape != proper_shape:\n",
    "        wrong_shape.append([log_rew_path, df_rew.shape])\n",
    "    \n",
    "    # Save file size\n",
    "    file_size.append((log_rew_path, os.path.getsize(log_pun_path)))\n",
    "    file_size.append((log_pun_path, os.path.getsize(log_pun_path)))\n",
    "    \n",
    "    # Look if duplicates are same\n",
    "    log_rew_path2 = f'{subfolder_path}/{subject}_prl_DecideNet_rew_2.csv'\n",
    "    log_pun_path2 = f'{subfolder_path}/{subject}_prl_DecideNet_pun_2.csv'\n",
    "    \n",
    "    if not filecmp.cmp(log_rew_path, log_rew_path2):\n",
    "        wrong_duplicates.append(log_rew_path)\n",
    "    if not filecmp.cmp(log_pun_path, log_pun_path2):\n",
    "        wrong_duplicates.append(log_pun_path)\n",
    "        \n",
    "print(f'--> Logs with wrong shape: \\n{wrong_shape}')\n",
    "print(f'--> Logs with wrong duplicates: \\n{wrong_duplicates}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix subject_id field and remove spurious columns\n",
    "Apply only after manually resolved conflicts with file names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    \n",
    "    subfolder_path = os.path.join(path_logs, 'sub-' + subject)\n",
    "    log_rew_path = f'{subfolder_path}/{subject}_prl_DecideNet_rew.csv'\n",
    "    log_pun_path = f'{subfolder_path}/{subject}_prl_DecideNet_pun.csv'\n",
    "    df_rew = pd.read_csv(log_rew_path)\n",
    "    df_pun = pd.read_csv(log_pun_path)\n",
    "    \n",
    "    # Filter only useful columns\n",
    "    keys = ['block', 'rwd', 'magn_left', 'magn_right', 'onset_iti_plan',\n",
    "       'onset_isi_plan', 'onset_dec_plan', 'onset_out_plan', '.thisRepN',\n",
    "       '.thisTrialN', '.thisN', '.thisIndex', 'onset_iti', 'onset_iti_glob',\n",
    "       'onset_dec', 'onset_dec_glob', 'onset_isi', 'onset_isi_glob',\n",
    "       'onset_out', 'onset_out_glob', 'acc_after_trial', 'won_bool',\n",
    "       'won_magn', 'rt', 'response', 'subject_id', 'condition', 'group']\n",
    "    df_rew = df_rew[keys]\n",
    "    df_pun = df_pun[keys]\n",
    "    \n",
    "    # Fix subject_id field\n",
    "    df_rew['subject_id'] = subject\n",
    "    df_pun['subject_id'] = subject\n",
    "\n",
    "    # Save changes\n",
    "    df_rew.to_csv(log_rew_path, index=False)\n",
    "    df_pun.to_csv(log_pun_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
