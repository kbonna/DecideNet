{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "\n",
    "path_root = os.environ.get('DECIDENET_PATH')\n",
    "path_code = os.path.join(path_root, 'code')\n",
    "if path_code not in sys.path:\n",
    "    sys.path.append(path_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pep_barplot(pep, model_names, outpath=None):\n",
    "    \"\"\"Create protected exceedance probability barplot.\n",
    "\n",
    "    Args:\n",
    "        pep: \n",
    "            protected exceedance probabilities for any number of models\n",
    "        modelname: \n",
    "            list of modelnames\n",
    "        outpath (optional):\n",
    "            path specification for saving figure\n",
    "    \"\"\"\n",
    "    n_models = len(pep)\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=1, nrows=1, facecolor='w', figsize=(1+n_models, 5))\n",
    "\n",
    "    ax.bar(range(n_models), pep, width=.65, color='gray')\n",
    "    ax.plot([-.5, n_models-.5], np.ones(2)*.95, color='r')\n",
    "\n",
    "    for i, val in enumerate(pep):\n",
    "        ax.text(i - .15, val + .02, f'{val:.3f}', \n",
    "                color='k')\n",
    "\n",
    "    ax.set_xlim([-.5, n_models-.5])\n",
    "    ax.set_xticks(range(n_models))\n",
    "    ax.set_xticklabels(model_names)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_title('Protected exceedance probability', pad=25)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid()\n",
    "\n",
    "    if outpath:\n",
    "        plt.savefig(outpath)\n",
    "        \n",
    "def gen_pmp_implot(pmp, modelnames, outpath=None, cmap='bone_r') -> None:\n",
    "    \"\"\"Create imagesc / imshow plot for posterior model probabilities.\n",
    "    \n",
    "    Args:\n",
    "        pmp: np.ndarray (n_models x n_subjects)\n",
    "            posterior model probabilities for arbitrary number of models\n",
    "        modelnames: list \n",
    "        outpath (optional): str\n",
    "            path specification for saving figure\n",
    "        cmap_name (optional): str\n",
    "    \"\"\"\n",
    "    pmp_norm = np.divide(pmp, np.mean(np.sum(pmp, 0)))\n",
    "    n_models = pmp_norm.shape[0]\n",
    "    n_subjects = pmp_norm.shape[1]\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, facecolor='w', \n",
    "                           figsize=(20, n_models+1))\n",
    "\n",
    "    im = ax.imshow(\n",
    "        pmp_norm, \n",
    "        aspect='auto',\n",
    "        cmap=cmap,\n",
    "        origin='upper'\n",
    "    )\n",
    "    im.set_clim(0, 1)\n",
    "\n",
    "    ax.set_title('Posterior model probability')\n",
    "\n",
    "    ax.set_xticks(np.arange(0, n_subjects, 1))\n",
    "    ax.set_xticks(np.arange(-.5, n_subjects, 1), minor=True)\n",
    "    ax.set_xticklabels([f'm{sub:02}' for sub in range(2, n_subjects+2)],\n",
    "                       rotation=-45)\n",
    "    ax.set_xlabel('Subjects')\n",
    "    ax.set_yticks(np.arange(0, n_models, 1))\n",
    "    ax.set_yticks(np.arange(-.5, n_models, 1), minor=True)\n",
    "    ax.set_ylim([n_models-.5, -.5])\n",
    "    ax.set_yticklabels(modelnames)\n",
    "    ax.set_ylabel('Model')\n",
    "    ax.grid(which='minor', color='k')\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax, pad=0.01)\n",
    "    plt.tight_layout()\n",
    "        \n",
    "    if outpath:\n",
    "        fig.savefig(outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "In this sections HLM model output is used for Bayesian model comparison. \n",
    "## Factorial Model\n",
    "### Prediction-error independent (PI) vs prediction-error dependent (PD)\n",
    "First, model indicator variable $z_i$ is used to discriminate between prediction-error independent (PI) and prediction-error dependent (PD) models. This approach called transdimensional MCMC encompasses both reversible jump MCMC and the product space technique (Sission, 2005). Bayes factors for PI vs PD model selection can be directly calculated for each participant as a ratio of model indicator samples. Bayes factor is the standard Bayesian solution to the model selection problems (Lewis & Raftery, 1997). Bayes factor (BF) for two models $M_1$ and $M_2$ is defined as:\n",
    "$$BF_{12}=\\frac{p(D|M_1)}{p(D|M_2)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_subjects' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-de92b86e2eef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath_vba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vba'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msublabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf'm{sub:02}'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msub\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_subjects\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodelnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'PICI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PICD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PDCI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PDCD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mn_subjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_subjects' is not defined"
     ]
    }
   ],
   "source": [
    "path_out = os.path.join(path_root, 'data/main_fmri_study/derivatives/jags')\n",
    "path_vba = os.path.join(path_out, 'vba')\n",
    "\n",
    "sublabels = [f'm{sub:02}' for sub in range(2, n_subjects+2)]\n",
    "modelnames = ['PICI', 'PICD', 'PDCI', 'PDCD']\n",
    "n_subjects = 32\n",
    "n_models = len(modelnames)\n",
    "\n",
    "# Load posterior samples\n",
    "path_hlm_sequential = os.path.join(path_out, 'jags_output/HLM_sequential_split.mat')\n",
    "mat = scipy.io.loadmat(path_hlm_sequential, variable_names=['samples'], squeeze_me=True)\n",
    "z = mat['samples']['z'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate posterior model probabilities\n",
    "pmp = np.concatenate(\n",
    "    tuple(np.sum((z == i) + (z == i+n_models) + (z == i+2*n_models), axis=(0, 1))[np.newaxis, :] \n",
    "          for i in range(1, 5)), \n",
    "    axis=0)\n",
    "pmp = pmp.astype(float)\n",
    "# pmp[pmp == 0] = np.finfo(float).eps \n",
    "\n",
    "# Save posterior model probabilities to *.mat format\n",
    "scipy.io.savemat(\n",
    "    file_name=os.path.join(path_vba, 'pmp_HLM_sequential_split.mat'), \n",
    "    mdict={'pmp': pmp}, \n",
    "    appendmat=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all subjects counts of z\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\n",
    "ax.hist(\n",
    "    z.flatten(), \n",
    "    np.arange(3 * n_models + 1) + .5,\n",
    "    rwidth=.75, \n",
    "    density=True,\n",
    "    color='gray'\n",
    ")\n",
    "ax.set_xticks(np.arange(1, 3 * n_models + 1))\n",
    "ax.set_xticklabels(3 * modelnames)\n",
    "ax.set_ylabel('Proportion couts')\n",
    "ax.set_title('Model indicator variable $z_i$ counts')\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_pmp_implot(pmp_seq, modelnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running code below run `dn_bm_03_vba.m` script to obtain posterior exceedance probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat(os.path.join(path_vba, 'pep_HLM_sequential_split.mat'), squeeze_me=True)\n",
    "gen_pep_barplot(mat['pep'], modelnames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
