model {

#
#   Latent behavioral variables:
#
#   wcor_l_, wcor_r_:   estimated probability of correct choice
#   exvl_l, exvl_r:   expected utility
#   theta:      	  choice probability for right box
#   
#   Loop variables:
#
#   i: 
#       subjects (range 1, 2, ..., 32)
#   j: 
#       task conditions (range 1, 2)
#           1: reward
#           2: punishment
#   t: 
#       trials (range 1, 2, ..., 110)
#

###########################################################################
## Generative model #######################################################
###########################################################################

for (i in 1:nSubjects){
    for (j in 1:nConditions){                                              

        #PDCI
        wcor_l_tmp_pdci[i, j, 1] = .5
        wcor_r_tmp_pdci[i, j, 1] = .5
        wcor_l_pdci[i, j, 1] = .5
        wcor_r_pdci[i, j, 1] = .5   
        exvl_l_pdci[i, j, 1] = ifelse(j==1, wcor_l_pdci[i, j, 1] * magnl[i, j, 1], (1-wcor_l_pdci[i, j, 1]) * magnl[i, j, 1])        
        exvl_r_pdci[i, j, 1] = ifelse(j==1, wcor_r_pdci[i, j, 1] * magnr[i, j, 1], (1-wcor_r_pdci[i, j, 1]) * magnr[i, j, 1])         
        theta[i, j, 1] = (1) / (1 + exp(beta_pdci[i] * (exvl_l_pdci[i, j, 1] - exvl_r_pdci[i, j, 1])))   
        respFictPDCI[i, j, 1] ~ dbern((1) / (1 + exp(beta_pdci[i] * (exvl_l_pdci[i, j, 1] - exvl_r_pdci[i, j, 1])))) # fictitious response

        # Generate response
        resp[i, j, 1] ~ dbern(theta[i, j, 1])

        for (t in 2:nTrials){
            
            # Update probability estimates (reinforcement learning)           

            #PDCI
            wcor_l_tmp_pdci[i, j, t] = ifelse(side[i, j, t-1] == 0,           
                wcor_l_pdci[i, j, t-1] + alpha_pdci[i, 1] * (1 - wcor_l_pdci[i, j, t-1]),
                wcor_l_pdci[i, j, t-1] + alpha_pdci[i, 2] * (0 - wcor_l_pdci[i, j, t-1]))
            wcor_r_tmp_pdci[i, j, t] = ifelse(side[i, j, t-1] == 1,
                wcor_r_pdci[i, j, t-1] + alpha_pdci[i, 1] * (1 - wcor_r_pdci[i, j, t-1]),
                wcor_r_pdci[i, j, t-1] + alpha_pdci[i, 2] * (0 - wcor_r_pdci[i, j, t-1]))
            wcor_l_pdci[i, j, t] = ifelse(respFictPDCI[i, j, t-1] == 0, 
                wcor_l_tmp_pdci[i, j, t], 
                1 - wcor_r_tmp_pdci[i, j, t])
            wcor_r_pdci[i, j, t] = ifelse(respFictPDCI[i, j, t-1] == 1, 
                wcor_r_tmp_pdci[i, j, t],
                1-wcor_l_tmp_pdci[i, j, t])

            # Recalculate utilities
            exvl_l_pdci[i, j, t] = ifelse(j==1, wcor_l_pdci[i, j, t] * magnl[i, j, t], (1-wcor_l_pdci[i, j, t]) * magnl[i, j, t])        
            exvl_r_pdci[i, j, t] = ifelse(j==1, wcor_r_pdci[i, j, t] * magnr[i, j, t], (1-wcor_r_pdci[i, j, t]) * magnr[i, j, t])            
            theta[i, j, t] = (1) / (1 + exp(beta_pdci[i] * (exvl_l_pdci[i, j, t] - exvl_r_pdci[i, j, t]))) 
            respFictPDCI[i, j, t] ~ dbern(max(0.000001, min(0.999999, (1) / (1 + exp(beta_pdci[i] * (exvl_l_pdci[i, j, t] - exvl_r_pdci[i, j, t]))))))   

            # Generate response
            resp[i, j, t] ~ dbern(max(0.000001, min(0.999999, theta[i, j, t])))    
            
        }#end trials
    }#end conditions
}#end subjects

###########################################################################
## Priors #################################################################
###########################################################################

# Behavioral priors
for (i in 1:nSubjects){

    #PDCI
    for (j in 1:nPredErrSign){
        alpha_pdci[i, j] ~ dbeta(a_alpha_pdci[j], b_alpha_pdci[j])
    }
    beta_pdci[i] ~ dlnorm(mu_beta_pdci, sigma_beta_pdci)

}#end subjects

###########################################################################
## Hyperpriors ############################################################
###########################################################################

#PDCI
for (j in 1:nPredErrSign){
    a_alpha_pdci[j] ~ dunif(1, 10)
    b_alpha_pdci[j] ~ dunif(1, 10)
}
mu_beta_pdci ~ dunif(-2.3, 3.4)
sigma_beta_pdci ~ dunif(0.01, 1.6) 

}
