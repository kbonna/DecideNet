model {

###########################################################################
## Generative model #######################################################
###########################################################################

for (i in 1:nSubjects){
    for (j in 1:nConditions){

        #all models
        vl_sl[i, j, 1]      = .5
        vr_sl[i, j, 1]      = .5
        vl_dlo[i, j, 1]     = .5
        vr_dlo[i, j, 1]     = .5
        vl_dlp[i, j, 1]     = .5
        vr_dlp[i, j, 1]     = .5        
        eul_sl[i, j, 1]     = vl_sl[i, j, 1] * xl[i, j, 1]        
        eur_sl[i, j, 1]     = vr_sl[i, j, 1] * xr[i, j, 1]         
        theta_sl[i, j, 1]   = (1) / (1 + exp(beta_sl[i] * (eul_sl[i, j, 1] - eur_sl[i, j, 1])))
        eul_dlo[i, j, 1]    = vl_dlo[i, j, 1] * xl[i, j, 1]        
        eur_dlo[i, j, 1]    = vr_dlo[i, j, 1] * xr[i, j, 1]         
        theta_dlo[i, j, 1]  = (1) / (1 + exp(beta_dlo[i] * (eul_dlo[i, j, 1] - eur_dlo[i, j, 1])))        
        eul_dlp[i, j, 1]    = vl_dlp[i, j, 1] * xl[i, j, 1]        
        eur_dlp[i, j, 1]    = vr_dlp[i, j, 1] * xr[i, j, 1]         
        theta_dlp[i, j, 1]  = (1) / (1 + exp(beta_dlp[i] * (eul_dlp[i, j, 1] - eur_dlp[i, j, 1])))      

        # Choose submodel & generate response
        # 1=SL; 2=DLO; 3=DPL
        theta[i, j, 1]      = ifelse(z[i]==1, theta_sl[i, j, 1], ifelse(z[i]==2, theta_dlo[i, j, 1], theta_dlp[i, j, 1])) 
        resp[i, j, 1]       ~ dbern(theta[i, j, 1])             

        for (t in 2:nTrials){
            
            # Update (reinforcement learning)
            #SL
            vl_sl[i, j, t]     = ifelse(
                rwd[i, j, t-1] == 0,                      
                vl_sl[i, j, t-1] + alpha_sl[i] * (1 - vl_sl[i, j, t-1]), 
                vl_sl[i, j, t-1] + alpha_sl[i] * (0 - vl_sl[i, j, t-1]))
            vr_sl[i, j, t]     = ifelse(                       
                rwd[i, j, t-1] == 1,                        
                vr_sl[i, j, t-1] + alpha_sl[i] * (1 - vr_sl[i, j, t-1]), 
                vr_sl[i, j, t-1] + alpha_sl[i] * (0 - vr_sl[i, j, t-1]))

            #DLO
            vl_dlo[i, j, t]     = ifelse(
                rwd[i, j, t-1] == 0,                   
                vl_dlo[i, j, t-1] + alpha_dlo[i, j] * (1 - vl_dlo[i, j, t-1]), 
                vl_dlo[i, j, t-1] + alpha_dlo[i, j] * (0 - vl_dlo[i, j, t-1]))
            vr_dlo[i, j, t]     = ifelse(                       
                rwd[i, j, t-1] == 1,                   
                vr_dlo[i, j, t-1] + alpha_dlo[i, j] * (1 - vr_dlo[i, j, t-1]), 
                vr_dlo[i, j, t-1] + alpha_dlo[i, j] * (0 - vr_dlo[i, j, t-1]))

            #DLP
            vl_dlp[i, j, t] = ifelse(j==1,  
                ifelse(                                                         #reward condition
                rwd[i, j, t-1] == 0,            
                vl_dlp[i, j, t-1] + alpha_dlp[i, 1] * (1 - vl_dlp[i, j, t-1]),  #left rewarded (+PE)
                vl_dlp[i, j, t-1] + alpha_dlp[i, 2] * (0 - vl_dlp[i, j, t-1])), #right rewarded (-PE)
                ifelse(                                                         #punishment condition
                rwd[i, j, t-1] == 0,
                vl_dlp[i, j, t-1] + alpha_dlp[i, 1] * (1 - vl_dlp[i, j, t-1]),  #right not punished (-PE)
                vl_dlp[i, j, t-1] + alpha_dlp[i, 2] * (0 - vl_dlp[i, j, t-1]))) #left not punished (+PE)
            vr_dlp[i, j, t] = ifelse(j==1,  
                ifelse(                                                         #reward condition
                rwd[i, j, t-1] == 1,            
                vr_dlp[i, j, t-1] + alpha_dlp[i, 1] * (1 - vr_dlp[i, j, t-1]),  #right rewarded (+PE)
                vr_dlp[i, j, t-1] + alpha_dlp[i, 2] * (0 - vr_dlp[i, j, t-1])), #left rewarded (-PE)
                ifelse(                                                         #punishment condition
                rwd[i, j, t-1] == 1,
                vr_dlp[i, j, t-1] + alpha_dlp[i, 1] * (1 - vr_dlp[i, j, t-1]),  #left not punished (-PE)
                vr_dlp[i, j, t-1] + alpha_dlp[i, 2] * (0 - vr_dlp[i, j, t-1]))) #right not punished (+PE)


            # Recalculate utilities
            eul_sl[i, j, t]     = vl_sl[i, j, t] * xl[i, j, t]        
            eur_sl[i, j, t]     = vr_sl[i, j, t] * xr[i, j, t]         
            theta_sl[i, j, t]   = (1) / (1 + exp(beta_sl[i] * (eul_sl[i, j, t] - eur_sl[i, j, t])))
            eul_dlo[i, j, t]    = vl_dlo[i, j, t] * xl[i, j, t]        
            eur_dlo[i, j, t]    = vr_dlo[i, j, t] * xr[i, j, t]         
            theta_dlo[i, j, t]  = (1) / (1 + exp(beta_dlo[i] * (eul_dlo[i, j, t] - eur_dlo[i, j, t])))        
            eul_dlp[i, j, t]    = vl_dlp[i, j, t] * xl[i, j, t]        
            eur_dlp[i, j, t]    = vr_dlp[i, j, t] * xr[i, j, t]         
            theta_dlp[i, j, t]  = (1) / (1 + exp(beta_dlp[i] * (eul_dlp[i, j, t] - eur_dlp[i, j, t])))      

            # Choose submodel & generate response
            tempth[i, j, t]     = ifelse(z[i]==1, theta_sl[i, j, t], ifelse(z[i]==2, theta_dlo[i, j, t], theta_dlp[i, j, t])) 
            theta[i, j, t]      = max(0.000001, min(0.999999, tempth[i, j, t])) # ensure 0 < cp < 1
            resp[i, j, t]       ~ dbern(theta[i, j, t])   
            
        }#end trials
    }#end conditions
}#end subjects

###########################################################################
## Priors #################################################################
###########################################################################

# Model indicator variable z can take on any value from 1:nModels, and is  
# subject to two stochastic processes, to prevent getting stuck
for (i in 1:nSubjects){    
    #px_z1[i]    ~ dcat(pz[])                                 #parameter expansion variable for z, takes on integers 1:n with equal probability
    #px_z2[i]    ~ dcat(pz[])                                 #parameter expansion variable for z, takes on integers 1:n with equal probability
    #delta_z1[i] = px_z2[i]-1                                 #parameter expansion variable for z, takes on integers 0:n-1 with equal probability
    #sum_z[i]    = px_z1[i]+delta_z1[i]                       #sum takes on integers 1:2*n -1 with equal probability
    #z[i]        = (sum_z[i] - (3 * trunc(sum_z[i]/3))) + 1   #modulo n, adding 1 to return to values 1 to 3 
    z[i]        ~ dcat(pz[])
}    

# Behavioral priors
for (i in 1:nSubjects){

    #SL
    alpha_sl[i]     ~ dbeta(a_alpha_sl, b_alpha_sl)
    beta_sl[i]      ~ dlnorm(mu_beta_sl, sigma_beta_sl)

    #DLO
    for (j in 1:nConditions){
        alpha_dlo[i, j] ~ dbeta(a_alpha_dlo[j], b_alpha_dlo[j])
    }
    beta_dlo[i]     ~ dlnorm(mu_beta_dlo, sigma_beta_dlo)

    #DLP 
    # 1=+PE, 2=-PE
    for (j in 1:2){
        alpha_dlp[i, j] ~ dbeta(a_alpha_dlp[j], b_alpha_dlp[j])
    }
    beta_dlp[i]     ~ dlnorm(mu_beta_dlp, sigma_beta_dlp)    
    
}#end subjects

###########################################################################
## Hyperpriors ############################################################
###########################################################################

#SL
a_alpha_sl          ~ dunif(1, 10)
b_alpha_sl          ~ dunif(1, 10)
mu_beta_sl          ~ dunif(-2.3, 3.4)
sigma_beta_sl       ~ dunif(0.01, 1.6) 

#DLO
for (j in 1:nConditions){
    a_alpha_dlo[j]  ~ dunif(1, 10)
    b_alpha_dlo[j]  ~ dunif(1, 10)
}
mu_beta_dlo         ~ dunif(-2.3, 3.4)
sigma_beta_dlo      ~ dunif(0.01, 1.6)

#DLP
for (j in 1:2){
    a_alpha_dlp[j]  ~ dunif(1, 10)
    b_alpha_dlp[j]  ~ dunif(1, 10)
}
mu_beta_dlp         ~ dunif(-2.3, 3.4)
sigma_beta_dlp      ~ dunif(0.01, 1.6)

}