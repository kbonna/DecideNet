{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The probabilistic reversal learning task\n",
    "\n",
    "During the fMRI scanning session participants carried out the PRL task in two conditions: (1) reward-seeking and (2) punishment-avoiding. Participants were instructed to repeatedly choose between yellow and blue boxes in order to collect as many points as possible in the reward-seeking condition or loose as little points as possible in the punishment-avoiding condition (**Fig. 1**). One of the boxes had the probability to be correct (rewarding or non-punishing depending on the condition) p = 0.8 and the other one p = 0.2. This reward contingency changed four times throughout each task condition. Reward probabilities were unknown to the subjects and had to be learned from experience. Each box had also associated reward magnitude, randomly selected at the beginning of each trial. These reward magnitudes represented as numbers within the box indicated possible gain in the reward-seeking condition or possible loss in the punishment-avoiding condition. To be successful in this task decision maker had to correctly estimate reward probabilities from experience and take into account reward magnitudes to choose an option with higher expected value. \n",
    "    \n",
    "Each task condition was associated with the separate fMRI run and consisted of N=110 trials. Each trial began with the decision phase indicated by the question mark appearing within the fixation circle. During decision phase subject had 2 s to choose one of the boxes by pressing button on the response grip with either left or right thumb. Decision phase was followed by a variable inter-stimulus-interval (ISI; 3-7 s, jittered), after which an outcome was presented for 2 s. During the outcome phase fixation circle was colored accordingly to rewarded or punished box and the number within the circle represented number of gained or lost points (see **Fig. 1**). Outcome phase was followed by a variable inter-trial-interval (ITI; 3-7 s, jittered). \n",
    "    \n",
    "    \n",
    "The number of points which subject gathered in the reward-seeking condition or the remaining number of points in the punishment-avoiding conditions was represented by the gray account bar on the bottom of the screen. Subjects were informed that if they manage to fill half of the bar or the entire bar in the reward-seeking condition they will receive 10 PLN or 20 PLN respectively. In the punishment-avoiding condition subjects were informed that they will receive 20 PLN if they are left with more than half of the bar, 10 PLN if they are left with less than half of the bar or that do not receive any money if they lose all of their points. To maintain constant level of motivation throughout the task, incentives thresholds were set such that all participants acquired 10 PLN from either task.\n",
    "\n",
    "\n",
    "PsychoPy software (v. 1.90.1, www.psychopy.org (Peirce, 2007)) was used for task presentation on the MRI compatible NNL goggles (NordicNeuroLab, Bergen, Norway). Behavioral responses were collected using MRI compatible NNL response grips (NordicNeuroLab, Bergen, Norway), which were hold in both hands.  Each condition lasted approximately 24 min. The order of task conditions as well as the colors for left and right box (yellow and blue) were counterbalanced across subjects. Before the MRI scan, subjects practiced both task conditions on the lab computer. \n",
    "Heterogeneity in the prior expectations regarding the task structure may lead to heterogeneity in behavior even in simple tasks leading to inaccurate behavioral modelling (Shteingart and Loewenstein, 2014). To tackle this challenge, we explicitly instructed participants that one of the boxes will be more frequently rewarded in the reward-seeking condition or punished in the punishment-avoiding condition and that this contingency may reverse several times during the task. In order to further ensure that participants grasp correct model of the task environment, they were provided with the feedback indicating which box is more frequently correct during the first phase of the training. \n",
    "\n",
    "![Figure 1](image/fig_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing of log dataframe\n",
    "\n",
    "Each row of a dataframe represents *single trial*.\n",
    "Description of useful dataframe columns:\n",
    "\n",
    "Task characteristics:\n",
    "- `block`: indicates which box is currently more likely to be rewarded / punished \n",
    "   - coding: -1 = left, 1 = right\n",
    "   - being chosen interpretation\n",
    "- `rwd`: indicates which box is rewarded / punished on a given trial\n",
    "   - coding: -1 = left, 1 = right\n",
    "   - being chosen interpretation\n",
    "- `magn_left` and `magn_right`: visible reward magnitudes\n",
    "\n",
    "Subject responses:\n",
    "- `response`: subject response\n",
    "   - coding: -1 = left, 0 = miss, 1 = right\n",
    "- `rt`: reaction time (max. 1.5 seconds)\n",
    "   - coding: nan for miss\n",
    "- `won_bool`: indicates if subject won / not loose points\n",
    "   - coding: 1 = won / not lost, 0 = lost / not won\n",
    "- `won_magn`: indicates number of points won / lost\n",
    "   - magnitude in points change (can be negative)\n",
    "- `acc_after_trial`: subject total points after given trial\n",
    "\n",
    "Task timing: \n",
    "- `onset_[iti/dec/isi/out]`: time registered with fmri clock\n",
    "    - used for analysis\n",
    "- `onset_[iti/dec/isi/out]_plan`: planned time for stimulus presentation (always slightly behind registered time)\n",
    "- `onset_[iti/dec/isi/out]_glob`: time registered with global clock (only for synchronization validation purpose, not used for analysis) \n",
    "\n",
    "Function `process_log_df()` cleans behavioral response dataframe changing data types, inverting interpretation of certain variables depending on task condition and drops irrelevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_df(df):\n",
    "    '''Cleaning and pre-processing of log dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.Dataframe): raw log dataframe\n",
    "        \n",
    "    Returns:\n",
    "        info (dictionary): contains task metadata\n",
    "        df_clean (pd.Dataframe): pre-processed log dataframe\n",
    "    '''\n",
    "    df_clean = df.copy(deep=True)\n",
    "    \n",
    "    # Convert subject responses to integers\n",
    "    df_clean.loc[df['response'] == 'a', 'response'] = -1\n",
    "    df_clean.loc[df['response'] == 'd', 'response'] = 1\n",
    "    try:\n",
    "        df_clean.loc[df_clean['response'] == 'None', 'response'] = 0\n",
    "    except TypeError:\n",
    "        print('No missing responses.')\n",
    "\n",
    "    # Convert reaction time to float\n",
    "    df_clean['rt'] = pd.to_numeric(df_clean['rt'], errors='coerce')\n",
    "\n",
    "    # Grab additional info                                                   \n",
    "    info = {}\n",
    "    info['n_trials'] = df_clean.shape[0]\n",
    "    info['n_blocks'] = 5                                                     #TODO grab from data\n",
    "    info['condition'] = df_clean['condition'][0]\n",
    "    info['subject'] = df_clean['subject_id'][0]\n",
    "    info['group'] = df_clean['group'][0]\n",
    "\n",
    "    # Reverse the punishing interpretation of columns in punishment condition \n",
    "    # NOTE! 'block' and 'rwd' still have BEING CHOSEN interpretation\n",
    "    if info['condition'] == 'pun':\n",
    "        df_clean['won_magn'] *= (-1)  \n",
    "        df_clean['won_bool'] = ~ df_clean['won_bool'] \n",
    "        df_clean['magn_left'] *= (-1)\n",
    "        df_clean['magn_right'] *= (-1)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df_clean = df_clean[['block', 'rwd', 'magn_left', 'magn_right',\n",
    "                         'response', 'rt',\n",
    "                         'won_bool', 'won_magn', 'acc_after_trial',\n",
    "                         'onset_iti', 'onset_iti_plan', 'onset_iti_glob',\n",
    "                         'onset_dec', 'onset_dec_plan', 'onset_dec_glob',\n",
    "                         'onset_isi', 'onset_isi_plan', 'onset_isi_glob',\n",
    "                         'onset_out', 'onset_out_plan', 'onset_out_glob']]\n",
    "\n",
    "    return info, df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) load logs for all subjects \n",
    "\n",
    "(2) cleaning them using `process_log_df()` function\n",
    "\n",
    "(3) aggregate them to single list containing all dataframes accompanied with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_logs = \"/home/kmb/Desktop/Neuroscience/Projects/BONNA_decide_net/data/main_fmri_study/task_logs\"\n",
    "subjects = ['m02', 'm03', 'm04', 'm05', 'm06', 'm07', 'm08', 'm09', 'm10', \n",
    "            'm11', 'm12', 'm13', 'm14', 'm15', 'm16', 'm17', 'm18', 'm19', \n",
    "            'm20', 'm21', 'm22', 'm23', 'm24', 'm25', 'm26', 'm27', 'm28', \n",
    "            'm29', 'm30', 'm31', 'm32', 'm33']\n",
    "df_all_rew, df_all_pun = [], []\n",
    "\n",
    "for subject in subjects:\n",
    "\n",
    "    # Load behavioral responses for signle subject\n",
    "    path_rew = f\"{path_logs}/sub-{subject}/{subject}_prl_DecideNet_rew.csv\"\n",
    "    path_pun = f\"{path_logs}/sub-{subject}/{subject}_prl_DecideNet_pun.csv\"\n",
    "\n",
    "    df_rew = pd.read_csv(path_rew)\n",
    "    df_pun = pd.read_csv(path_pun)\n",
    "\n",
    "    # Clean behavioral responses\n",
    "    info_rew, df_rew = process_log_df(df_rew)\n",
    "    info_pun, df_pun = process_log_df(df_pun)\n",
    "\n",
    "    df_all_rew.append((info_rew, df_rew))\n",
    "    df_all_pun.append((info_pun, df_pun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of subject responses\n",
    "Function `plot_response()` creates friendly visualisation of subject responses throughout the task. Visualisation consists of:\n",
    "- **top panel**: represents internal task structure, blue and yellow blocks show stable phases for which box reward probabilities do not change (color is coding more profitable side; blue=left, yellow=right), dark blue line show reward magnitude for the left box, yellow and blue dots show rewarded / punished sides\n",
    "- **middle panel**: represents subject's reaction times (red line) and account balance (black dashed line) throughout the task, red coloring also highlight trials with missing response\n",
    "- **bottom panel**: represetnes subject's trialwise responses and outcome of each trial (green dot = rewarded / not punished; red dot = not rewarded / punished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_response(df, info, save=False, **kwargs):\n",
    "    '''Visualising useful aspects of subjects responses.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.Dataframe): clean log dataframe\n",
    "        info (dictionary): contains task metadata\n",
    "        save (Bool): should I save your plot?\n",
    "        ...\n",
    "        **out_path (Str): path to folder to save plot\n",
    "    '''\n",
    "    col_blu = \"#56B4E9\" # left\n",
    "    col_yel = \"#F0E442\" # right\n",
    "    col_blu_d = \"#0B3A54\"\n",
    "\n",
    "    # Determine begin and end of the blocks and rewarded side\n",
    "    N_blocks = info['n_blocks']\n",
    "    N_trials = info['n_trials']\n",
    "    condition = info['condition']\n",
    "\n",
    "    blocks = np.zeros((2, N_blocks+1), dtype='int')\n",
    "    blocks[0, 0:N_blocks] = np.nonzero(df['block'].diff() != 0)[0] + 1\n",
    "    blocks[0, N_blocks] = N_trials\n",
    "    blocks[1, 0:N_blocks] = df.loc[blocks[0], 'block'][:-1]\n",
    "\n",
    "    if condition == 'pun':\n",
    "        blocks[1, :] *= (-1) \n",
    "\n",
    "    rt = pd.to_numeric(df['rt'], errors='coerce')\n",
    "\n",
    "    x_trials = np.arange(1, N_trials+1)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, \n",
    "                                        figsize=(20, 10), facecolor='w')\n",
    "\n",
    "    ### Subplot 1 ###########################################################\n",
    "    # True reward contingencies (more profitable side)\n",
    "    for i in range(N_blocks):\n",
    "\n",
    "        if blocks[1, i] == -1:  col = col_blu\n",
    "        else:                   col = col_yel\n",
    "\n",
    "        ax1.fill_between(\n",
    "            x=[blocks[0, i], blocks[0, i+1]], \n",
    "            y1=-1, y2=1, \n",
    "            color=col, alpha=.5)\n",
    "\n",
    "    # Rewarded / punished side\n",
    "    ax1.scatter(x_trials, df.rwd*.9, c=df.rwd, \n",
    "                cmap='cividis', vmin=-1.5, vmax=1.5)\n",
    "    ax1.set_ylim(-1, 1)\n",
    "    ax1.set_yticklabels([])\n",
    "    \n",
    "    # Magnitude for left box\n",
    "    ax1b = ax1.twinx()\n",
    "    ax1b.plot(x_trials, df['magn_left'], color=col_blu_d)\n",
    "    ax1b.set_ylabel('$x(t)$ for left box', color=col_blu_d)\n",
    "    ax1b.set_xlim(1, N_trials)\n",
    "    if condition == 'pun': \n",
    "        ax1b.set_ylim(-50, 0)\n",
    "    else:               \n",
    "        ax1b.set_ylim(0, 50)\n",
    "\n",
    "    ### Subplot 2 ###########################################################\n",
    "    # Misses\n",
    "    for miss in np.argwhere(np.isnan(rt)):\n",
    "\n",
    "        ax2.fill_between(\n",
    "            x=[miss[0]+.5, miss[0]+1.5],\n",
    "            y1=0, y2=1.5,\n",
    "            color='r', alpha=.5)\n",
    "    ax1b.set_xlim(1, N_trials)\n",
    "\n",
    "    # Reaction times\n",
    "    ax2.plot(x_trials, pd.to_numeric(df['rt'], errors='coerce'), 'r')\n",
    "    ax2.set_ylabel('reaction time $[s]$', color='r')\n",
    "    ax2.set_xticks(blocks[0, 1:-1])\n",
    "    ax2.set_ylim(0, 1.5)\n",
    "    ax2.grid(axis='x')\n",
    "\n",
    "    # Account\n",
    "    ax2b = ax2.twinx()\n",
    "    ax2b.plot(x_trials, df.acc_after_trial, 'k--')\n",
    "    ax2b.set_ylabel('account balance')\n",
    "\n",
    "    ### Subplot 3 ###########################################################\n",
    "    ax3.scatter(x_trials, df.response*.75, c=df['won_bool'], \n",
    "                cmap='RdYlGn', vmin=-.2, vmax=1.2)\n",
    "    ax3.set_ylim(-1, 1)\n",
    "    ax3.set_yticks([-0.75, 0, .75])\n",
    "    ax3.set_yticklabels(['left', 'miss', 'right'])\n",
    "    ax3.grid(axis='both')\n",
    "    \n",
    "    if save:\n",
    "        \n",
    "        if \"out_path\" in kwargs:  out_path = kwargs[\"out_path\"] + \"/\"\n",
    "        else:                     out_path = \"\"\n",
    "            \n",
    "        filename = f\"{out_path}sub-{info['subject']}_{info['condition']}_respplot\"\n",
    "        plt.savefig(filename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/home/kmb/Desktop/Neuroscience/Projects/\"\\\n",
    "           \"BONNA_decide_net/code/beh_modelling/respplots\"\n",
    "\n",
    "for i in range(len(subjects)):\n",
    "    # Save respplots to file\n",
    "    plot_response(df_all_rew[i][1], df_all_rew[i][0], \n",
    "                  save=True, out_path=out_path);\n",
    "    plot_response(df_all_pun[i][1], df_all_pun[i][0], \n",
    "                  save=True, out_path=out_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create single variable to represent all behavioral responses\n",
    "Use aggregated lists of clean dataframes (`df_all_rew` and `dr_all_pun`) and convert them to single numpy array representing all behavioral responses and task onsets. Size of aggregated array is: n_subjects x n_conditions x n_trials x 21. Array metadata decoding dimensions is stored in variable `meta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/home/kmb/Desktop/Neuroscience/Projects/BONNA_decide_net/data/main_fmri_study/behavioral\"\n",
    "filename = \"behavioral_data_clean_all\"\n",
    "\n",
    "beh = np.zeros((len(subjects), 2, 110, 21))\n",
    "\n",
    "# create & save metadata\n",
    "meta = {}\n",
    "meta['dim1'] = subjects\n",
    "meta['dim2'] = ['rew', 'pun']\n",
    "meta['dim3'] = [f'trial_{i+1}' for i in range(110)]\n",
    "meta['dim4'] = list(df_all_rew[0][1].keys())\n",
    "\n",
    "meta_path = os.path.join(out_path, f\"{filename}.json\")\n",
    "with open(meta_path, 'w') as f:\n",
    "    json.dump(meta, f, indent=4)\n",
    "\n",
    "# create & save numpy aggregated array\n",
    "for i, (df_rew, df_pun) in enumerate(zip(df_all_rew, df_all_pun)):\n",
    "    beh[i, 0] = np.array(df_rew[1], dtype='float')\n",
    "    beh[i, 1] = np.array(df_pun[1], dtype='float')\n",
    "    \n",
    "beh_path = os.path.join(out_path, f\"{filename}.npy\")\n",
    "np.save(beh_path, beh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
