{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The probabilistic reversal learning task\n",
    "\n",
    "During the fMRI scanning session participants carried out the PRL task in two conditions: (1) reward-seeking and (2) punishment-avoiding. Participants were instructed to repeatedly choose between yellow and blue boxes in order to collect as many points as possible in the reward-seeking condition or loose as little points as possible in the punishment-avoiding condition (**Fig. 1**). One of the boxes had the probability to be correct (rewarding or non-punishing depending on the condition) p = 0.8 and the other one p = 0.2. This reward contingency changed four times throughout each task condition. Reward probabilities were unknown to the subjects and had to be learned from experience. Each box had also associated reward magnitude, randomly selected at the beginning of each trial. These reward magnitudes represented as numbers within the box indicated possible gain in the reward-seeking condition or possible loss in the punishment-avoiding condition. To be successful in this task decision maker had to correctly estimate reward probabilities from experience and take into account reward magnitudes to choose an option with higher expected value. \n",
    "    \n",
    "Each task condition was associated with the separate fMRI run and consisted of N=110 trials. Each trial began with the decision phase indicated by the question mark appearing within the fixation circle. During decision phase subject had 2 s to choose one of the boxes by pressing button on the response grip with either left or right thumb. Decision phase was followed by a variable inter-stimulus-interval (ISI; 3-7 s, jittered), after which an outcome was presented for 2 s. During the outcome phase fixation circle was colored accordingly to rewarded or punished box and the number within the circle represented number of gained or lost points (see **Fig. 1**). Outcome phase was followed by a variable inter-trial-interval (ITI; 3-7 s, jittered). \n",
    "    \n",
    "    \n",
    "The number of points which subject gathered in the reward-seeking condition or the remaining number of points in the punishment-avoiding conditions was represented by the gray account bar on the bottom of the screen. Subjects were informed that if they manage to fill half of the bar or the entire bar in the reward-seeking condition they will receive 10 PLN or 20 PLN respectively. In the punishment-avoiding condition subjects were informed that they will receive 20 PLN if they are left with more than half of the bar, 10 PLN if they are left with less than half of the bar or that do not receive any money if they lose all of their points. To maintain constant level of motivation throughout the task, incentives thresholds were set such that all participants acquired 10 PLN from either task.\n",
    "\n",
    "\n",
    "PsychoPy software (v. 1.90.1, www.psychopy.org (Peirce, 2007)) was used for task presentation on the MRI compatible NNL goggles (NordicNeuroLab, Bergen, Norway). Behavioral responses were collected using MRI compatible NNL response grips (NordicNeuroLab, Bergen, Norway), which were hold in both hands.  Each condition lasted approximately 24 min. The order of task conditions as well as the colors for left and right box (yellow and blue) were counterbalanced across subjects. Before the MRI scan, subjects practiced both task conditions on the lab computer. \n",
    "Heterogeneity in the prior expectations regarding the task structure may lead to heterogeneity in behavior even in simple tasks leading to inaccurate behavioral modelling (Shteingart and Loewenstein, 2014). To tackle this challenge, we explicitly instructed participants that one of the boxes will be more frequently rewarded in the reward-seeking condition or punished in the punishment-avoiding condition and that this contingency may reverse several times during the task. In order to further ensure that participants grasp correct model of the task environment, they were provided with the feedback indicating which box is more frequently correct during the first phase of the training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral data conventions\n",
    "\n",
    "## Files\n",
    "Behavioral data is stored in two separate files `behavioral_data_clean_all.npy` and `behavioral_data_clean_all.json`. File with *.npy extension is array containing actual data and corresponding *.json file contains names of corresponding fields. \n",
    "1. Aggregated behavioral data loaded from `behavioral_data_clean_all.npy` is called is represented as `beh` variable in the code. \n",
    "2. Corresponding metadata dictionary is called as `meta` variable in the code. \n",
    "\n",
    "## Fields\n",
    "Aggregated behavioral data contains **all task information, all behavioral responses  / recordings and timing of all scanner events**.  Multidimensional array `beh` aggregates data for subjects, conditions, trials for different events. Shape of the array is $N_{subjects} \\times N_{conditions} \\times N_{trials} \\times N_{variables}$. \n",
    "\n",
    "### Task structure variables\n",
    "| variable name | code | values |\n",
    "|--|--|--|\n",
    "| Correct side (block) | `block` | -1 / 1 |\n",
    "| Chosen side (block) | `block_bci` | -1 / 1 |\n",
    "| Correct side | `side` | -1 / 1 |\n",
    "| Chosen side | `side_bci` | -1 / 1 |\n",
    "| Left-side magnitude | `magn_left` | `int` from -45 to 45 |\n",
    "| Right-side magnitude | `magn_right` | `int` from -45 to 45 |\n",
    "\n",
    "Note the distinction between correct and chosen side. Correct is related to human utility interpretation where correct means *rewarded or not punished*. Chosen is related to **being chosen interpretation** where chosen means *rewarded or punished* (it is used for RL algorithm calculations). In the reward condition all sides that are chosen are correct at the same time, whereas in the punishment condition all sides that are chosen are incorrect. In different parts of the code, different interpretation are used, so they are separately defined to avoid confusion.\n",
    "\n",
    "### Subject's behavioral variables\n",
    "| variable name | code | values |\n",
    "|--|--|--|\n",
    "| Response | `response` | -1 / 0 / 1 |\n",
    "| Reaction time (s) | `rt` | `float` from 0 to 1.5 / `nan` |\n",
    "| Correct choice label | `won_bool` | 0 / 1 |\n",
    "| Account update | `won_magn` | `int` from -45 to 45 |\n",
    "| Account balance (after trial) | `acc_after_trial` | `int` from 0 to 2300 |\n",
    "\n",
    "Response variable is coded the same way as in task structure variables. It is often desirable to convert -1 / 1 coding into 0 / 1 coding, as it is more convenient for JAGS to represent direct probabilities that one side will be chosen. However, in order to incorporate intuitive representation of missing responses as 0, default coding is -1 / 0 / 1 for left / miss / right. \n",
    "\n",
    "### Scanner timing variables\n",
    "| variable name | code |\n",
    "|--|--|\n",
    "| main fmri clock time (used for analysis)|`onset_[iti/dec/isi/out]`|\n",
    "| planned time of stimulus presentation (planned time for stimulus presentation (always slightly behind registered time)| `onset_[iti/dec/isi/out]_plan`|\n",
    "| time registered with global clock (only for synchronization validation purpose, not used for analysis) |  `onset_[iti/dec/isi/out]_glob` \n",
    "\n",
    "Subscripts represent different events related with single trial. `iti` reflects inter-trial-interval onset (before each trial), `dec` is decision phase onset, `isi` is inter-stimulus-interval onset (waiting phase onset) and `out` is outcome phase onset.\n",
    "\n",
    "## Latent algorithm variables\n",
    "\n",
    "### Algorithm parameters\n",
    "| variable name | code | text |\n",
    "|--|--|--|\n",
    "| learning rate | `alpha` | $\\alpha$ |\n",
    "| learning rate for positive PE | `alpha_plus` / `alpha[first_index]` | $\\alpha_+$ |\n",
    "| learning rate for negative PE | `alpha_minus` / `alpha[second_index]` | $\\alpha_-$ |\n",
    "| learning rate for reward condition | `alpha_rew` / `alpha[first_index]` | $\\alpha_{rew}$ |\n",
    "| learning rate for punishment condition | `alpha_pun` / `alpha[second_index]` | $\\alpha_{pun}$ |\n",
    "| inverse-temperature | `beta` | $\\beta$ |\n",
    "| loss aversion (prospect theory) | `gamma` | $\\gamma$ |\n",
    "| risk aversion (prospect theory) | `delta` | $\\delta$ |\n",
    "\n",
    "Note that learning rates can be represented either as separate vectors or arrays with columns corresponding to different learning rates. `first_index` and `second_index` are used instead of specific values because in Python `first_index` is 0, whereas in MATLAB it is 1 (and so on). In case of four learning rates model first dimension corresponds to task condition and second dimension corresponds to PE sign. \n",
    "\n",
    "\n",
    "### Tracked and computed variables\n",
    "| variable name | code | text |\n",
    "|--|--|--|\n",
    "| Expected probability for side for being chosen | `wch` / `wch_l` and `wch_r` | $\\rho$ |\n",
    "| Expected probability for side for being correct | `wco` / `wco_l` and `wco_r` | $p$|\n",
    "| Expected value (utility) | `util` / `util_l` and `util_r` | $v$|\n",
    "| Reward magnitude | `magn` / `magn_l` and `magn_r` | $x$|\n",
    "| Choice probability | `prob` / `prob_l` and `prob_r` | $P$|\n",
    "\n",
    "Whenever `l` and `r` suffixes are not used, **array representation** of tracked variables is assumed. In array representation $N_{trials} \\times N_{sides}$ array is representing variable state across task time course for both sides simultaneously. \n",
    "\n",
    "\n",
    "Function `process_log_df()` cleans behavioral response dataframe changing data types, inverting interpretation of certain variables depending on task condition and drops irrelevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_log_df(df):\n",
    "    '''Cleaning and pre-processing of log dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.Dataframe): raw log dataframe\n",
    "        \n",
    "    Returns:\n",
    "        info (dictionary): contains task metadata\n",
    "        df_clean (pd.Dataframe): pre-processed log dataframe\n",
    "    '''\n",
    "\n",
    "    df_clean = df.copy(deep=True)\n",
    "\n",
    "    # Grab additional info                                                   \n",
    "    info = {}\n",
    "    info['n_trials'] = df_clean.shape[0]\n",
    "    info['n_blocks'] = 5                                                     \n",
    "    info['condition'] = df_clean['condition'][0]\n",
    "    info['subject'] = df_clean['subject_id'][0]\n",
    "    info['group'] = df_clean['group'][0]\n",
    "\n",
    "    # Reaname non-intuitive columns according to guidlines\n",
    "    df_clean.rename(columns={'block': 'block_bci'}, inplace=True)\n",
    "    df_clean.rename(columns={'rwd': 'side_bci'}, inplace=True)\n",
    "\n",
    "    if info['condition'] == 'pun':\n",
    "        df_clean['block'] = (-1) * df_clean['block_bci']  \n",
    "        df_clean['side'] = (-1) * df_clean['side_bci']\n",
    "    else:\n",
    "        df_clean['block'] = df_clean['block_bci']  \n",
    "        df_clean['side'] = df_clean['side_bci']\n",
    "\n",
    "\n",
    "    # Convert subject responses to integers\n",
    "    df_clean.loc[df['response'] == 'a', 'response'] = -1\n",
    "    df_clean.loc[df['response'] == 'd', 'response'] = 1\n",
    "    try:\n",
    "        df_clean.loc[df_clean['response'] == 'None', 'response'] = 0\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Convert reaction time to float\n",
    "    df_clean['rt'] = pd.to_numeric(df_clean['rt'], errors='coerce')\n",
    "\n",
    "    # Reverse incorrect sign for punishment variables \n",
    "    if info['condition'] == 'pun':\n",
    "        df_clean['won_bool'] = ~ df_clean['won_bool'] \n",
    "        df_clean['won_magn'] *= (-1)  \n",
    "        df_clean['magn_left'] *= (-1)\n",
    "        df_clean['magn_right'] *= (-1)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df_clean = df_clean[['block', 'block_bci', 'side', 'side_bci', \n",
    "                         'magn_left', 'magn_right',\n",
    "                         'response', 'rt', \n",
    "                         'won_bool', 'won_magn', 'acc_after_trial',\n",
    "                         'onset_iti', 'onset_iti_plan', 'onset_iti_glob',\n",
    "                         'onset_dec', 'onset_dec_plan', 'onset_dec_glob',\n",
    "                         'onset_isi', 'onset_isi_plan', 'onset_isi_glob',\n",
    "                         'onset_out', 'onset_out_plan', 'onset_out_glob']]\n",
    "\n",
    "    return info, df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) load logs for all subjects \n",
    "\n",
    "(2) cleaning them using `process_log_df()` function\n",
    "\n",
    "(3) aggregate them to single list containing all dataframes accompanied with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_logs = '/home/kmb/Desktop/Neuroscience/Projects/BONNA_decide_net/' \\\n",
    "            'data/main_fmri_study/sourcedata/behavioral/task_logs'\n",
    "\n",
    "subjects = [f'm{sub:02}' for sub in range(2, 34)]\n",
    "\n",
    "df_all_rew, df_all_pun = [], []\n",
    "\n",
    "for subject in subjects:\n",
    "\n",
    "    # Load behavioral responses for signle subject\n",
    "    path_rew = f\"{path_logs}/sub-{subject}/{subject}_prl_DecideNet_rew.csv\"\n",
    "    path_pun = f\"{path_logs}/sub-{subject}/{subject}_prl_DecideNet_pun.csv\"\n",
    "\n",
    "    df_rew = pd.read_csv(path_rew)\n",
    "    df_pun = pd.read_csv(path_pun)\n",
    "\n",
    "    # Clean behavioral responses\n",
    "    info_rew, df_rew = process_log_df(df_rew)\n",
    "    info_pun, df_pun = process_log_df(df_pun)\n",
    "\n",
    "    df_all_rew.append((info_rew, df_rew))\n",
    "    df_all_pun.append((info_pun, df_pun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create single variable to represent all behavioral responses\n",
    "Use aggregated lists of clean dataframes (`df_all_rew` and `dr_all_pun`) and convert them to single numpy array representing all behavioral responses and task onsets. Size of aggregated array is: n_subjects x n_conditions x n_trials x 21. Array metadata decoding dimensions is stored in variable `meta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_path = '/home/kmb/Desktop/Neuroscience/Projects/BONNA_decide_net/' \\\n",
    "           'data/main_fmri_study/sourcedata/behavioral'\n",
    "filename = \"behavioral_data_clean_all_REF\"\n",
    "\n",
    "beh = np.zeros((len(subjects), 2, 110, 23))\n",
    "\n",
    "# create & save metadata\n",
    "meta = {}\n",
    "meta['dim1'] = subjects\n",
    "meta['dim2'] = ['rew', 'pun']\n",
    "meta['dim3'] = [f'trial_{i+1}' for i in range(110)]\n",
    "meta['dim4'] = list(df_all_rew[0][1].keys())\n",
    "\n",
    "meta_path = os.path.join(out_path, f\"{filename}.json\")\n",
    "with open(meta_path, 'w') as f:\n",
    "    json.dump(meta, f, indent=4)\n",
    "\n",
    "# create & save numpy aggregated array\n",
    "for i, (df_rew, df_pun) in enumerate(zip(df_all_rew, df_all_pun)):\n",
    "    beh[i, 0] = np.array(df_rew[1], dtype='float')\n",
    "    beh[i, 1] = np.array(df_pun[1], dtype='float')\n",
    "    \n",
    "beh_path = os.path.join(out_path, f\"{filename}.npy\")\n",
    "np.save(beh_path, beh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of subject responses\n",
    "Function `plot_response()` creates friendly visualisation of subject responses throughout the task. Visualisation consists of:\n",
    "- **top panel**: represents internal task structure\n",
    "    - blue and yellow blocks show stable phases for which box reward probabilities do not change (color is coding more profitable side; blue=left, yellow=right)\n",
    "    - dark blue line show reward magnitude for the left box\n",
    "    - yellow and blue dots show winning sides (rewarded / not punished)\n",
    "- **middle panel**: represents subject's reaction times and account balance\n",
    "    - red line: reaction time, \n",
    "    - red rectangles: highlight misses\n",
    "    - black dashed line: account balance throughout the task\n",
    "    - dark shaded area: trials for which subject crossed reward threshold\n",
    "- **bottom panel**: represetnes subject's trialwise responses \n",
    "    - green dot = rewarded / not punished; red dot = not rewarded / punished\n",
    "    - dark dashed line: idle time (how many stable trials subject experienced)\n",
    "    - colored rectangles: which side is more profitable in terms of reward magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load aggregated behavioral data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/kmb/Desktop/Neuroscience/Projects/BONNA_decide_net/code')\n",
    "from dn_utils.behavioral_models_REF import load_behavioral_data\n",
    "\n",
    "beh_path = \"/home/kmb/Desktop/Neuroscience/Projects/BONNA_decide_net/\" \\\n",
    "           \"data/main_fmri_study/sourcedata/behavioral/\"\n",
    "beh_meta = load_behavioral_data(beh_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_response(beh, meta, subject, condition, save=False, **kwargs):\n",
    "    '''Visualising useful aspects of subjects responses.\n",
    "    \n",
    "    Args:\n",
    "        beh (np.array): aggregated behavioral responses\n",
    "        meta (dict): description of beh array coding\n",
    "        subject (int): subject index\n",
    "        condition (int): task condition index\n",
    "            0 for reward condition or 1 for punishment condition\n",
    "        save (bool): should I save your plot?\n",
    "        ...\n",
    "        **out_path (Str): path to folder to save plot\n",
    "    '''\n",
    "    col_blu = \"#56B4E9\" # left\n",
    "    col_yel = \"#F0E442\" # right\n",
    "    col_blu_d = \"#0B3A54\"\n",
    "    \n",
    "    # Get proper task & response features\n",
    "    block_bci = beh[subject, condition, :, meta['dim4'].index('block_bci')]\n",
    "    side_bci = beh[subject, condition, :, meta['dim4'].index('side_bci')]\n",
    "    magn_left = beh[subject, condition, :, meta['dim4'].index('magn_left')]\n",
    "    magn_right = beh[subject, condition, :, meta['dim4'].index('magn_right')]\n",
    "    response = beh[subject, condition, :, meta['dim4'].index('response')]\n",
    "    rt = beh[subject, condition, :, meta['dim4'].index('rt')]\n",
    "    won_bool = beh[subject, condition, :, meta['dim4'].index('won_bool')]\n",
    "    acc_after_trial = beh[subject, condition, :, meta['dim4'].index('acc_after_trial')]\n",
    "    magn_diff = magn_right - magn_left\n",
    "\n",
    "    n_blocks = np.nonzero(np.diff(block_bci))[0].shape[0] + 1\n",
    "    n_trials = beh.shape[2]\n",
    "    x_trials = np.arange(1, n_trials+1)\n",
    "    \n",
    "    # Determine begin and end of the blocks and rewarded side\n",
    "    blocks = np.zeros((2, n_blocks+1), dtype='int')\n",
    "    blocks[0, 0:n_blocks] = np.hstack((\n",
    "        np.ones((1), dtype=int), \n",
    "        np.nonzero(np.diff(beh[subject, condition, :, 0]))[0] + 2\n",
    "    ))\n",
    "    blocks[0, n_blocks] = n_trials\n",
    "    blocks[1, 0:n_blocks] = beh[subject, condition, blocks[0][:-1], 0]\n",
    "\n",
    "    if condition == 1: blocks[1, :] *= (-1) \n",
    "\n",
    "    # Create plot\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, \n",
    "                                        figsize=(20, 10), facecolor='w')\n",
    "\n",
    "    ### Subplot 1 ###########################################################\n",
    "    # True reward contingencies (more profitable side)\n",
    "    for i in range(n_blocks):\n",
    "\n",
    "        if blocks[1, i] == -1:  col = col_blu\n",
    "        else:                   col = col_yel\n",
    "\n",
    "        ax1.fill_between(\n",
    "            x=[blocks[0, i], blocks[0, i+1]], \n",
    "            y1=-1, y2=1, \n",
    "            color=col, alpha=.5)\n",
    "\n",
    "    # Rewarded / punished side\n",
    "    if condition == 1:    \n",
    "        ax1.scatter(x_trials, (-1)*side_bci*.9, c=(-1)*side_bci, \n",
    "                    cmap='cividis', vmin=-1.5, vmax=1.5)\n",
    "    else: \n",
    "        ax1.scatter(x_trials, side_bci*.9, c=side_bci, \n",
    "                    cmap='cividis', vmin=-1.5, vmax=1.5)\n",
    "    ax1.set_ylim(-1, 1)\n",
    "    ax1.set_yticks([-0.9, 0.9])\n",
    "    ax1.set_yticklabels(['left', 'right'])\n",
    "    ax1.set_ylabel('Better option')\n",
    "\n",
    "    # Magnitude for left box\n",
    "    ax1b = ax1.twinx()\n",
    "    ax1b.plot(x_trials, magn_left, color=col_blu_d)\n",
    "    ax1b.set_ylabel('$x(t)$ for left box', color=col_blu_d)\n",
    "    ax1b.set_xlim(1, n_trials)\n",
    "    if condition == 1: \n",
    "        ax1b.set_ylim(-50, 0)\n",
    "    else:               \n",
    "        ax1b.set_ylim(0, 50)\n",
    "    ax1b.set_xlim(1, n_trials)\n",
    "\n",
    "    ### Subplot 2 ###########################################################\n",
    "    # Misses\n",
    "    for miss in np.argwhere(np.isnan(rt)):\n",
    "\n",
    "        ax2.fill_between(\n",
    "            x=[miss[0]+.5, miss[0]+1.5],\n",
    "            y1=0, y2=1.5,\n",
    "            color='r', alpha=.5)\n",
    "\n",
    "    # Reaction times\n",
    "    ax2.plot(x_trials, rt, 'r')\n",
    "    ax2.set_ylabel('reaction time $[s]$', color='r')\n",
    "    ax2.set_xticks(blocks[0, 1:-1])\n",
    "    ax2.set_ylim(0, 1.5)\n",
    "    ax2.grid(axis='x')\n",
    "\n",
    "    # Account\n",
    "    ax2b = ax2.twinx()\n",
    "    ax2b.plot(x_trials, acc_after_trial, 'k--')\n",
    "    ax2b.set_ylabel('account balance')\n",
    "\n",
    "    # Crossing predefined task threshold\n",
    "    if condition == 1:\n",
    "        acc_thr = np.ones(x_trials.shape) * 650\n",
    "        ax2b.fill_between(x_trials, acc_after_trial, acc_thr, \n",
    "                          where=acc_after_trial <=acc_thr,\n",
    "                          color='k', alpha=.2)\n",
    "    else:\n",
    "        acc_thr = np.ones(x_trials.shape) * 1150\n",
    "        ax2b.fill_between(x_trials, acc_after_trial, acc_thr, \n",
    "                      where=acc_after_trial >=acc_thr,\n",
    "                      color='k', alpha=.2)\n",
    "\n",
    "    ### Subplot 3 ###########################################################\n",
    "    # Idle time (repeated winning / not loosing side)\n",
    "    idle = np.zeros(side_bci.shape)\n",
    "    for i in range(1, len(side_bci)):\n",
    "        current = side_bci[i]\n",
    "        last_trials = np.flip(side_bci[:i] == current)\n",
    "        t = 0\n",
    "        while last_trials[t] == True: \n",
    "            t += 1\n",
    "            if t == len(last_trials): \n",
    "                break\n",
    "        if condition == 1:  idle[i] = t * current * (-1)\n",
    "        else:               idle[i] = t * current\n",
    "\n",
    "    ax3.plot(x_trials, idle, 'k')\n",
    "    ax3.set_ylim(-np.max(np.abs(idle)) - 2, np.max(np.abs(idle)) + 2)\n",
    "    ax3.set_ylabel('Idle time')\n",
    "\n",
    "    # Difference in magnitude\n",
    "    norm = matplotlib.colors.Normalize(-45, 45)\n",
    "    colors = [[norm(-45), col_blu],\n",
    "              [norm(0), \"white\"],\n",
    "              [norm(45), col_yel]]\n",
    "    cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n",
    "\n",
    "    ax3b = ax3.twinx()\n",
    "    for trial in range(n_trials):\n",
    "        ax3b.fill_between(\n",
    "            x=[trial+.5, trial+1.5], \n",
    "            y1=-1, y2=1, \n",
    "            color=cmap(norm(magn_diff[trial])), alpha=.7)\n",
    "\n",
    "    # Subject respnses\n",
    "    ax3b.scatter(x_trials, response*.75, c=won_bool, \n",
    "                cmap='RdYlGn', vmin=-.2, vmax=1.2, s=50)\n",
    "    ax3b.set_ylim(-1, 1)\n",
    "    ax3b.set_yticks([-0.75, 0, .75])\n",
    "    ax3b.set_yticklabels(['left', 'miss', 'right'])\n",
    "    ax3b.grid(axis='both')    \n",
    "    ax3b.spines['top'].set_color(col_yel)\n",
    "    ax3b.spines['top'].set_linewidth(3)\n",
    "    ax3b.spines['bottom'].set_color(col_blu)\n",
    "    ax3b.spines['bottom'].set_linewidth(3)\n",
    "\n",
    "    if save:\n",
    "\n",
    "        if \"out_path\" in kwargs:  out_path = kwargs[\"out_path\"] + \"/\"\n",
    "        else:                     out_path = \"\"\n",
    "\n",
    "        filename = f\"{out_path}sub-{meta['dim1'][subject]}_{meta['dim2'][condition]}_respplot\"\n",
    "        plt.savefig(filename)    \n",
    "        \n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show example plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_response(beh, meta, 5, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and save response plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_path = \"/home/kmb/Desktop/Neuroscience/Projects/\"\\\n",
    "           \"BONNA_decide_net/code/behavioral_analysis/figures/respplots\"\n",
    "\n",
    "for i in range(n_subjects):\n",
    "    # Save respplots to file\n",
    "    plot_response(beh, meta, i, 0, \n",
    "                  save=True, out_path=out_path);\n",
    "    plot_response(beh, meta, i, 1,\n",
    "                  save=True, out_path=out_path);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
